{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3941f3bf-9405-464f-9d2a-61e3dc2a6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "url = 'https://raw.githubusercontent.com/gregversteeg/LinearCorex/master/tests/data/test_big5.csv'\n",
    "df = pd.read_csv(url)\n",
    "df = df / 4.0  # Normalize to [0, 1]\n",
    "data = df.values.astype(np.float32)  # Shape: (num_samples, 50)\n",
    "\n",
    "# Convert data to torch tensors\n",
    "data_tensor = torch.tensor(data)\n",
    "dataset = TensorDataset(data_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a898f473-953d-440a-8d76-ec98e00d7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vector Quantizer class\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Flatten input\n",
    "        inputs = inputs.view(-1, self.embedding_dim)\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = (torch.sum(inputs ** 2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self.embedding.weight ** 2, dim=1)\n",
    "                     - 2 * torch.matmul(inputs, self.embedding.weight.t()))\n",
    "        \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.size(0), self.num_embeddings, device=device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight)\n",
    "        quantized = quantized.view(inputs.shape)\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = torch.mean((quantized.detach() - inputs) ** 2)\n",
    "        q_latent_loss = torch.mean((quantized - inputs.detach()) ** 2)\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Straight Through Estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        \n",
    "        # Compute perplexity\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        return loss, quantized, perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7412a1b2-e457-4f76-8063-9d7f83eeae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc_layers = nn.Sequential()\n",
    "        in_dim = input_dim\n",
    "        for i, h_dim in enumerate(hidden_dims):\n",
    "            self.fc_layers.add_module(f\"fc_{i}\", nn.Linear(in_dim, h_dim))\n",
    "            self.fc_layers.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        # Output layer with embedding_dim now set to 5\n",
    "        self.fc_layers.add_module(\"fc_latent\", nn.Linear(in_dim, embedding_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z_e = self.fc_layers(x)\n",
    "        return z_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf5ea02-2898-44d0-93d5-eb18a0c9c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, embedding_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc_layers = nn.Sequential()\n",
    "        in_dim = embedding_dim  # Input dimension is now 5\n",
    "        for i, h_dim in enumerate(hidden_dims):\n",
    "            self.fc_layers.add_module(f\"fc_{i}\", nn.Linear(in_dim, h_dim))\n",
    "            self.fc_layers.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "        # Output layer to reconstruct input_dim features\n",
    "        self.fc_layers.add_module(\"fc_output\", nn.Linear(in_dim, output_dim))\n",
    "        self.fc_layers.add_module(\"sigmoid\", nn.Sigmoid())  # Output between 0 and 1\n",
    "\n",
    "    def forward(self, z_q):\n",
    "        x_recon = self.fc_layers(z_q)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a28f1eb-07f7-467c-8b15-089974865a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, embedding_dim, num_embeddings, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dims, embedding_dim)\n",
    "        self.quantizer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "        self.decoder = Decoder(input_dim, hidden_dims[::-1], embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        loss, z_q, perplexity = self.quantizer(z_e)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return loss, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e63fbb-8cad-4757-b60e-38bea5ca4695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Batch [0/63] Loss: 0.1278, Recon Loss: 0.1214, VQ Loss: 0.0064, Perplexity: 2.2360\n",
      "Epoch [1/100] Batch [10/63] Loss: 0.1217, Recon Loss: 0.1211, VQ Loss: 0.0006, Perplexity: 13.0288\n",
      "Epoch [1/100] Batch [20/63] Loss: 0.1143, Recon Loss: 0.1140, VQ Loss: 0.0003, Perplexity: 12.5305\n",
      "Epoch [1/100] Batch [30/63] Loss: 0.1087, Recon Loss: 0.1083, VQ Loss: 0.0003, Perplexity: 5.4630\n",
      "Epoch [1/100] Batch [40/63] Loss: 0.1110, Recon Loss: 0.1103, VQ Loss: 0.0007, Perplexity: 1.6679\n",
      "Epoch [1/100] Batch [50/63] Loss: 0.1030, Recon Loss: 0.1026, VQ Loss: 0.0004, Perplexity: 1.6679\n",
      "Epoch [1/100] Batch [60/63] Loss: 0.1050, Recon Loss: 0.1042, VQ Loss: 0.0008, Perplexity: 1.7548\n",
      "====> Epoch: 1 Average loss: 0.1100\n",
      "Epoch [2/100] Batch [0/63] Loss: 0.0908, Recon Loss: 0.0899, VQ Loss: 0.0009, Perplexity: 1.6202\n",
      "Epoch [2/100] Batch [10/63] Loss: 0.1041, Recon Loss: 0.1028, VQ Loss: 0.0013, Perplexity: 1.6910\n",
      "Epoch [2/100] Batch [20/63] Loss: 0.0937, Recon Loss: 0.0924, VQ Loss: 0.0013, Perplexity: 1.6910\n",
      "Epoch [2/100] Batch [30/63] Loss: 0.0867, Recon Loss: 0.0851, VQ Loss: 0.0016, Perplexity: 3.0171\n",
      "Epoch [2/100] Batch [40/63] Loss: 0.0799, Recon Loss: 0.0787, VQ Loss: 0.0012, Perplexity: 3.8264\n",
      "Epoch [2/100] Batch [50/63] Loss: 0.1037, Recon Loss: 0.1013, VQ Loss: 0.0024, Perplexity: 6.2014\n",
      "Epoch [2/100] Batch [60/63] Loss: 0.0883, Recon Loss: 0.0856, VQ Loss: 0.0027, Perplexity: 4.4246\n",
      "====> Epoch: 2 Average loss: 0.0951\n",
      "Epoch [3/100] Batch [0/63] Loss: 0.0945, Recon Loss: 0.0916, VQ Loss: 0.0029, Perplexity: 3.3125\n",
      "Epoch [3/100] Batch [10/63] Loss: 0.0971, Recon Loss: 0.0935, VQ Loss: 0.0036, Perplexity: 3.7090\n",
      "Epoch [3/100] Batch [20/63] Loss: 0.0809, Recon Loss: 0.0758, VQ Loss: 0.0051, Perplexity: 6.0612\n",
      "Epoch [3/100] Batch [30/63] Loss: 0.0824, Recon Loss: 0.0777, VQ Loss: 0.0047, Perplexity: 5.2564\n",
      "Epoch [3/100] Batch [40/63] Loss: 0.0943, Recon Loss: 0.0871, VQ Loss: 0.0073, Perplexity: 3.5145\n",
      "Epoch [3/100] Batch [50/63] Loss: 0.0935, Recon Loss: 0.0833, VQ Loss: 0.0102, Perplexity: 4.4263\n",
      "Epoch [3/100] Batch [60/63] Loss: 0.1004, Recon Loss: 0.0875, VQ Loss: 0.0129, Perplexity: 3.7253\n",
      "====> Epoch: 3 Average loss: 0.0934\n",
      "Epoch [4/100] Batch [0/63] Loss: 0.1097, Recon Loss: 0.0975, VQ Loss: 0.0122, Perplexity: 4.4448\n",
      "Epoch [4/100] Batch [10/63] Loss: 0.1067, Recon Loss: 0.0992, VQ Loss: 0.0075, Perplexity: 4.2386\n",
      "Epoch [4/100] Batch [20/63] Loss: 0.1056, Recon Loss: 0.0943, VQ Loss: 0.0113, Perplexity: 3.7253\n",
      "Epoch [4/100] Batch [30/63] Loss: 0.0931, Recon Loss: 0.0860, VQ Loss: 0.0071, Perplexity: 4.3779\n",
      "Epoch [4/100] Batch [40/63] Loss: 0.0993, Recon Loss: 0.0867, VQ Loss: 0.0126, Perplexity: 4.0486\n",
      "Epoch [4/100] Batch [50/63] Loss: 0.1037, Recon Loss: 0.0870, VQ Loss: 0.0167, Perplexity: 6.3301\n",
      "Epoch [4/100] Batch [60/63] Loss: 0.0894, Recon Loss: 0.0806, VQ Loss: 0.0088, Perplexity: 4.7486\n",
      "====> Epoch: 4 Average loss: 0.0962\n",
      "Epoch [5/100] Batch [0/63] Loss: 0.0923, Recon Loss: 0.0811, VQ Loss: 0.0113, Perplexity: 3.8452\n",
      "Epoch [5/100] Batch [10/63] Loss: 0.1073, Recon Loss: 0.0870, VQ Loss: 0.0204, Perplexity: 4.8757\n",
      "Epoch [5/100] Batch [20/63] Loss: 0.0888, Recon Loss: 0.0812, VQ Loss: 0.0076, Perplexity: 4.9423\n",
      "Epoch [5/100] Batch [30/63] Loss: 0.0889, Recon Loss: 0.0817, VQ Loss: 0.0072, Perplexity: 6.0671\n",
      "Epoch [5/100] Batch [40/63] Loss: 0.1004, Recon Loss: 0.0876, VQ Loss: 0.0128, Perplexity: 6.1617\n",
      "Epoch [5/100] Batch [50/63] Loss: 0.0942, Recon Loss: 0.0843, VQ Loss: 0.0099, Perplexity: 4.8312\n",
      "Epoch [5/100] Batch [60/63] Loss: 0.0885, Recon Loss: 0.0813, VQ Loss: 0.0072, Perplexity: 6.9299\n",
      "====> Epoch: 5 Average loss: 0.0942\n",
      "Epoch [6/100] Batch [0/63] Loss: 0.0841, Recon Loss: 0.0794, VQ Loss: 0.0047, Perplexity: 5.1001\n",
      "Epoch [6/100] Batch [10/63] Loss: 0.0904, Recon Loss: 0.0834, VQ Loss: 0.0070, Perplexity: 4.6490\n",
      "Epoch [6/100] Batch [20/63] Loss: 0.0811, Recon Loss: 0.0746, VQ Loss: 0.0064, Perplexity: 6.2191\n",
      "Epoch [6/100] Batch [30/63] Loss: 0.1019, Recon Loss: 0.0902, VQ Loss: 0.0117, Perplexity: 4.6555\n",
      "Epoch [6/100] Batch [40/63] Loss: 0.0938, Recon Loss: 0.0853, VQ Loss: 0.0085, Perplexity: 5.7606\n",
      "Epoch [6/100] Batch [50/63] Loss: 0.0879, Recon Loss: 0.0790, VQ Loss: 0.0090, Perplexity: 5.2366\n",
      "Epoch [6/100] Batch [60/63] Loss: 0.0858, Recon Loss: 0.0743, VQ Loss: 0.0116, Perplexity: 4.2765\n",
      "====> Epoch: 6 Average loss: 0.0880\n",
      "Epoch [7/100] Batch [0/63] Loss: 0.0830, Recon Loss: 0.0760, VQ Loss: 0.0070, Perplexity: 6.8299\n",
      "Epoch [7/100] Batch [10/63] Loss: 0.0776, Recon Loss: 0.0767, VQ Loss: 0.0009, Perplexity: 6.9547\n",
      "Epoch [7/100] Batch [20/63] Loss: 0.0806, Recon Loss: 0.0785, VQ Loss: 0.0021, Perplexity: 6.4853\n",
      "Epoch [7/100] Batch [30/63] Loss: 0.0751, Recon Loss: 0.0714, VQ Loss: 0.0037, Perplexity: 8.2810\n",
      "Epoch [7/100] Batch [40/63] Loss: 0.0789, Recon Loss: 0.0744, VQ Loss: 0.0045, Perplexity: 7.4642\n",
      "Epoch [7/100] Batch [50/63] Loss: 0.0919, Recon Loss: 0.0876, VQ Loss: 0.0043, Perplexity: 7.1319\n",
      "Epoch [7/100] Batch [60/63] Loss: 0.0796, Recon Loss: 0.0729, VQ Loss: 0.0067, Perplexity: 7.8636\n",
      "====> Epoch: 7 Average loss: 0.0835\n",
      "Epoch [8/100] Batch [0/63] Loss: 0.0819, Recon Loss: 0.0742, VQ Loss: 0.0077, Perplexity: 10.0683\n",
      "Epoch [8/100] Batch [10/63] Loss: 0.0899, Recon Loss: 0.0845, VQ Loss: 0.0054, Perplexity: 6.5922\n",
      "Epoch [8/100] Batch [20/63] Loss: 0.0699, Recon Loss: 0.0667, VQ Loss: 0.0032, Perplexity: 9.6206\n",
      "Epoch [8/100] Batch [30/63] Loss: 0.0860, Recon Loss: 0.0809, VQ Loss: 0.0051, Perplexity: 7.5408\n",
      "Epoch [8/100] Batch [40/63] Loss: 0.0707, Recon Loss: 0.0695, VQ Loss: 0.0012, Perplexity: 8.9356\n",
      "Epoch [8/100] Batch [50/63] Loss: 0.0831, Recon Loss: 0.0751, VQ Loss: 0.0080, Perplexity: 10.0592\n",
      "Epoch [8/100] Batch [60/63] Loss: 0.0708, Recon Loss: 0.0656, VQ Loss: 0.0052, Perplexity: 9.9529\n",
      "====> Epoch: 8 Average loss: 0.0803\n",
      "Epoch [9/100] Batch [0/63] Loss: 0.0775, Recon Loss: 0.0721, VQ Loss: 0.0053, Perplexity: 8.7289\n",
      "Epoch [9/100] Batch [10/63] Loss: 0.0815, Recon Loss: 0.0764, VQ Loss: 0.0051, Perplexity: 9.3849\n",
      "Epoch [9/100] Batch [20/63] Loss: 0.0897, Recon Loss: 0.0789, VQ Loss: 0.0108, Perplexity: 7.9932\n",
      "Epoch [9/100] Batch [30/63] Loss: 0.0861, Recon Loss: 0.0799, VQ Loss: 0.0062, Perplexity: 10.8303\n",
      "Epoch [9/100] Batch [40/63] Loss: 0.0774, Recon Loss: 0.0764, VQ Loss: 0.0010, Perplexity: 13.5980\n",
      "Epoch [9/100] Batch [50/63] Loss: 0.0824, Recon Loss: 0.0800, VQ Loss: 0.0024, Perplexity: 12.2296\n",
      "Epoch [9/100] Batch [60/63] Loss: 0.0753, Recon Loss: 0.0704, VQ Loss: 0.0049, Perplexity: 10.9459\n",
      "====> Epoch: 9 Average loss: 0.0785\n",
      "Epoch [10/100] Batch [0/63] Loss: 0.0689, Recon Loss: 0.0660, VQ Loss: 0.0029, Perplexity: 12.2672\n",
      "Epoch [10/100] Batch [10/63] Loss: 0.0735, Recon Loss: 0.0721, VQ Loss: 0.0014, Perplexity: 10.9558\n",
      "Epoch [10/100] Batch [20/63] Loss: 0.0761, Recon Loss: 0.0716, VQ Loss: 0.0045, Perplexity: 13.9697\n",
      "Epoch [10/100] Batch [30/63] Loss: 0.0762, Recon Loss: 0.0738, VQ Loss: 0.0024, Perplexity: 13.2722\n",
      "Epoch [10/100] Batch [40/63] Loss: 0.0783, Recon Loss: 0.0759, VQ Loss: 0.0024, Perplexity: 13.5980\n",
      "Epoch [10/100] Batch [50/63] Loss: 0.0739, Recon Loss: 0.0717, VQ Loss: 0.0022, Perplexity: 14.9546\n",
      "Epoch [10/100] Batch [60/63] Loss: 0.0773, Recon Loss: 0.0749, VQ Loss: 0.0024, Perplexity: 13.5980\n",
      "====> Epoch: 10 Average loss: 0.0777\n",
      "Epoch [11/100] Batch [0/63] Loss: 0.0685, Recon Loss: 0.0670, VQ Loss: 0.0014, Perplexity: 12.2407\n",
      "Epoch [11/100] Batch [10/63] Loss: 0.0730, Recon Loss: 0.0667, VQ Loss: 0.0063, Perplexity: 15.4852\n",
      "Epoch [11/100] Batch [20/63] Loss: 0.0770, Recon Loss: 0.0741, VQ Loss: 0.0030, Perplexity: 12.5305\n",
      "Epoch [11/100] Batch [30/63] Loss: 0.0762, Recon Loss: 0.0719, VQ Loss: 0.0043, Perplexity: 13.2435\n",
      "Epoch [11/100] Batch [40/63] Loss: 0.0635, Recon Loss: 0.0593, VQ Loss: 0.0042, Perplexity: 13.1037\n",
      "Epoch [11/100] Batch [50/63] Loss: 0.0901, Recon Loss: 0.0836, VQ Loss: 0.0065, Perplexity: 8.9163\n",
      "Epoch [11/100] Batch [60/63] Loss: 0.0808, Recon Loss: 0.0786, VQ Loss: 0.0022, Perplexity: 17.1651\n",
      "====> Epoch: 11 Average loss: 0.0779\n",
      "Epoch [12/100] Batch [0/63] Loss: 0.0881, Recon Loss: 0.0826, VQ Loss: 0.0055, Perplexity: 13.1320\n",
      "Epoch [12/100] Batch [10/63] Loss: 0.0731, Recon Loss: 0.0668, VQ Loss: 0.0063, Perplexity: 12.3334\n",
      "Epoch [12/100] Batch [20/63] Loss: 0.0790, Recon Loss: 0.0756, VQ Loss: 0.0034, Perplexity: 13.5980\n",
      "Epoch [12/100] Batch [30/63] Loss: 0.0770, Recon Loss: 0.0710, VQ Loss: 0.0060, Perplexity: 13.2722\n",
      "Epoch [12/100] Batch [40/63] Loss: 0.0901, Recon Loss: 0.0815, VQ Loss: 0.0087, Perplexity: 13.1605\n",
      "Epoch [12/100] Batch [50/63] Loss: 0.0762, Recon Loss: 0.0706, VQ Loss: 0.0057, Perplexity: 13.8221\n",
      "Epoch [12/100] Batch [60/63] Loss: 0.0761, Recon Loss: 0.0697, VQ Loss: 0.0064, Perplexity: 14.7967\n",
      "====> Epoch: 12 Average loss: 0.0787\n",
      "Epoch [13/100] Batch [0/63] Loss: 0.0688, Recon Loss: 0.0637, VQ Loss: 0.0050, Perplexity: 14.9870\n",
      "Epoch [13/100] Batch [10/63] Loss: 0.0734, Recon Loss: 0.0696, VQ Loss: 0.0039, Perplexity: 15.2341\n",
      "Epoch [13/100] Batch [20/63] Loss: 0.0886, Recon Loss: 0.0774, VQ Loss: 0.0112, Perplexity: 10.7721\n",
      "Epoch [13/100] Batch [30/63] Loss: 0.0741, Recon Loss: 0.0665, VQ Loss: 0.0076, Perplexity: 14.5882\n",
      "Epoch [13/100] Batch [40/63] Loss: 0.0747, Recon Loss: 0.0690, VQ Loss: 0.0057, Perplexity: 11.4657\n",
      "Epoch [13/100] Batch [50/63] Loss: 0.0762, Recon Loss: 0.0670, VQ Loss: 0.0091, Perplexity: 10.8871\n",
      "Epoch [13/100] Batch [60/63] Loss: 0.0691, Recon Loss: 0.0633, VQ Loss: 0.0058, Perplexity: 12.1444\n",
      "====> Epoch: 13 Average loss: 0.0793\n",
      "Epoch [14/100] Batch [0/63] Loss: 0.0929, Recon Loss: 0.0801, VQ Loss: 0.0128, Perplexity: 12.3334\n",
      "Epoch [14/100] Batch [10/63] Loss: 0.0769, Recon Loss: 0.0724, VQ Loss: 0.0045, Perplexity: 15.2012\n",
      "Epoch [14/100] Batch [20/63] Loss: 0.0776, Recon Loss: 0.0718, VQ Loss: 0.0058, Perplexity: 16.0784\n",
      "Epoch [14/100] Batch [30/63] Loss: 0.0786, Recon Loss: 0.0709, VQ Loss: 0.0077, Perplexity: 13.0215\n",
      "Epoch [14/100] Batch [40/63] Loss: 0.0704, Recon Loss: 0.0631, VQ Loss: 0.0073, Perplexity: 14.8287\n",
      "Epoch [14/100] Batch [50/63] Loss: 0.0792, Recon Loss: 0.0738, VQ Loss: 0.0054, Perplexity: 12.9073\n",
      "Epoch [14/100] Batch [60/63] Loss: 0.0725, Recon Loss: 0.0675, VQ Loss: 0.0050, Perplexity: 12.9470\n",
      "====> Epoch: 14 Average loss: 0.0778\n",
      "Epoch [15/100] Batch [0/63] Loss: 0.0721, Recon Loss: 0.0674, VQ Loss: 0.0047, Perplexity: 15.3967\n",
      "Epoch [15/100] Batch [10/63] Loss: 0.0852, Recon Loss: 0.0786, VQ Loss: 0.0066, Perplexity: 12.7095\n",
      "Epoch [15/100] Batch [20/63] Loss: 0.0737, Recon Loss: 0.0687, VQ Loss: 0.0050, Perplexity: 14.2896\n",
      "Epoch [15/100] Batch [30/63] Loss: 0.0611, Recon Loss: 0.0581, VQ Loss: 0.0031, Perplexity: 14.3206\n",
      "Epoch [15/100] Batch [40/63] Loss: 0.0750, Recon Loss: 0.0695, VQ Loss: 0.0055, Perplexity: 12.3981\n",
      "Epoch [15/100] Batch [50/63] Loss: 0.0696, Recon Loss: 0.0649, VQ Loss: 0.0047, Perplexity: 15.2341\n",
      "Epoch [15/100] Batch [60/63] Loss: 0.0786, Recon Loss: 0.0745, VQ Loss: 0.0041, Perplexity: 16.3434\n",
      "====> Epoch: 15 Average loss: 0.0754\n",
      "Epoch [16/100] Batch [0/63] Loss: 0.0851, Recon Loss: 0.0773, VQ Loss: 0.0079, Perplexity: 17.0670\n",
      "Epoch [16/100] Batch [10/63] Loss: 0.0643, Recon Loss: 0.0608, VQ Loss: 0.0034, Perplexity: 19.2304\n",
      "Epoch [16/100] Batch [20/63] Loss: 0.0594, Recon Loss: 0.0562, VQ Loss: 0.0033, Perplexity: 16.3081\n",
      "Epoch [16/100] Batch [30/63] Loss: 0.0702, Recon Loss: 0.0665, VQ Loss: 0.0036, Perplexity: 21.3167\n",
      "Epoch [16/100] Batch [40/63] Loss: 0.0767, Recon Loss: 0.0748, VQ Loss: 0.0019, Perplexity: 16.3434\n",
      "Epoch [16/100] Batch [50/63] Loss: 0.0671, Recon Loss: 0.0655, VQ Loss: 0.0016, Perplexity: 24.6754\n",
      "Epoch [16/100] Batch [60/63] Loss: 0.0820, Recon Loss: 0.0732, VQ Loss: 0.0088, Perplexity: 16.6129\n",
      "====> Epoch: 16 Average loss: 0.0743\n",
      "Epoch [17/100] Batch [0/63] Loss: 0.0775, Recon Loss: 0.0593, VQ Loss: 0.0181, Perplexity: 16.6129\n",
      "Epoch [17/100] Batch [10/63] Loss: 0.0725, Recon Loss: 0.0688, VQ Loss: 0.0037, Perplexity: 14.9870\n",
      "Epoch [17/100] Batch [20/63] Loss: 0.0715, Recon Loss: 0.0672, VQ Loss: 0.0043, Perplexity: 15.0732\n",
      "Epoch [17/100] Batch [30/63] Loss: 0.0758, Recon Loss: 0.0697, VQ Loss: 0.0061, Perplexity: 19.7561\n",
      "Epoch [17/100] Batch [40/63] Loss: 0.0716, Recon Loss: 0.0699, VQ Loss: 0.0017, Perplexity: 19.2304\n",
      "Epoch [17/100] Batch [50/63] Loss: 0.0727, Recon Loss: 0.0693, VQ Loss: 0.0035, Perplexity: 18.4151\n",
      "Epoch [17/100] Batch [60/63] Loss: 0.0738, Recon Loss: 0.0691, VQ Loss: 0.0048, Perplexity: 19.8697\n",
      "====> Epoch: 17 Average loss: 0.0734\n",
      "Epoch [18/100] Batch [0/63] Loss: 0.0667, Recon Loss: 0.0622, VQ Loss: 0.0046, Perplexity: 18.9185\n",
      "Epoch [18/100] Batch [10/63] Loss: 0.0744, Recon Loss: 0.0732, VQ Loss: 0.0012, Perplexity: 22.2604\n",
      "Epoch [18/100] Batch [20/63] Loss: 0.0804, Recon Loss: 0.0762, VQ Loss: 0.0043, Perplexity: 22.2604\n",
      "Epoch [18/100] Batch [30/63] Loss: 0.0690, Recon Loss: 0.0665, VQ Loss: 0.0025, Perplexity: 20.0818\n",
      "Epoch [18/100] Batch [40/63] Loss: 0.0732, Recon Loss: 0.0672, VQ Loss: 0.0060, Perplexity: 21.3167\n",
      "Epoch [18/100] Batch [50/63] Loss: 0.0742, Recon Loss: 0.0686, VQ Loss: 0.0056, Perplexity: 20.4129\n",
      "Epoch [18/100] Batch [60/63] Loss: 0.0795, Recon Loss: 0.0778, VQ Loss: 0.0018, Perplexity: 21.3167\n",
      "====> Epoch: 18 Average loss: 0.0725\n",
      "Epoch [19/100] Batch [0/63] Loss: 0.0728, Recon Loss: 0.0700, VQ Loss: 0.0028, Perplexity: 16.3434\n",
      "Epoch [19/100] Batch [10/63] Loss: 0.0733, Recon Loss: 0.0700, VQ Loss: 0.0033, Perplexity: 17.9251\n",
      "Epoch [19/100] Batch [20/63] Loss: 0.0756, Recon Loss: 0.0727, VQ Loss: 0.0029, Perplexity: 16.1708\n",
      "Epoch [19/100] Batch [30/63] Loss: 0.0730, Recon Loss: 0.0697, VQ Loss: 0.0033, Perplexity: 18.9185\n",
      "Epoch [19/100] Batch [40/63] Loss: 0.0769, Recon Loss: 0.0715, VQ Loss: 0.0055, Perplexity: 17.3484\n",
      "Epoch [19/100] Batch [50/63] Loss: 0.0709, Recon Loss: 0.0645, VQ Loss: 0.0064, Perplexity: 21.6681\n",
      "Epoch [19/100] Batch [60/63] Loss: 0.0599, Recon Loss: 0.0570, VQ Loss: 0.0029, Perplexity: 24.2752\n",
      "====> Epoch: 19 Average loss: 0.0718\n",
      "Epoch [20/100] Batch [0/63] Loss: 0.0707, Recon Loss: 0.0658, VQ Loss: 0.0049, Perplexity: 15.2341\n",
      "Epoch [20/100] Batch [10/63] Loss: 0.0747, Recon Loss: 0.0690, VQ Loss: 0.0057, Perplexity: 15.6167\n",
      "Epoch [20/100] Batch [20/63] Loss: 0.0754, Recon Loss: 0.0722, VQ Loss: 0.0032, Perplexity: 19.7561\n",
      "Epoch [20/100] Batch [30/63] Loss: 0.0690, Recon Loss: 0.0674, VQ Loss: 0.0017, Perplexity: 21.8994\n",
      "Epoch [20/100] Batch [40/63] Loss: 0.0656, Recon Loss: 0.0620, VQ Loss: 0.0036, Perplexity: 22.2604\n",
      "Epoch [20/100] Batch [50/63] Loss: 0.0694, Recon Loss: 0.0675, VQ Loss: 0.0019, Perplexity: 21.8994\n",
      "Epoch [20/100] Batch [60/63] Loss: 0.0676, Recon Loss: 0.0634, VQ Loss: 0.0042, Perplexity: 20.4129\n",
      "====> Epoch: 20 Average loss: 0.0717\n",
      "Epoch [21/100] Batch [0/63] Loss: 0.0812, Recon Loss: 0.0795, VQ Loss: 0.0017, Perplexity: 21.3167\n",
      "Epoch [21/100] Batch [10/63] Loss: 0.0698, Recon Loss: 0.0677, VQ Loss: 0.0022, Perplexity: 20.7494\n",
      "Epoch [21/100] Batch [20/63] Loss: 0.0743, Recon Loss: 0.0702, VQ Loss: 0.0041, Perplexity: 18.7187\n",
      "Epoch [21/100] Batch [30/63] Loss: 0.0869, Recon Loss: 0.0853, VQ Loss: 0.0016, Perplexity: 17.1651\n",
      "Epoch [21/100] Batch [40/63] Loss: 0.0768, Recon Loss: 0.0759, VQ Loss: 0.0008, Perplexity: 19.7561\n",
      "Epoch [21/100] Batch [50/63] Loss: 0.0651, Recon Loss: 0.0639, VQ Loss: 0.0012, Perplexity: 19.5475\n",
      "Epoch [21/100] Batch [60/63] Loss: 0.0791, Recon Loss: 0.0728, VQ Loss: 0.0063, Perplexity: 18.8777\n",
      "====> Epoch: 21 Average loss: 0.0709\n",
      "Epoch [22/100] Batch [0/63] Loss: 0.0674, Recon Loss: 0.0648, VQ Loss: 0.0026, Perplexity: 22.6274\n",
      "Epoch [22/100] Batch [10/63] Loss: 0.0704, Recon Loss: 0.0672, VQ Loss: 0.0032, Perplexity: 19.5475\n",
      "Epoch [22/100] Batch [20/63] Loss: 0.0650, Recon Loss: 0.0574, VQ Loss: 0.0075, Perplexity: 16.8867\n",
      "Epoch [22/100] Batch [30/63] Loss: 0.0767, Recon Loss: 0.0737, VQ Loss: 0.0030, Perplexity: 17.3484\n",
      "Epoch [22/100] Batch [40/63] Loss: 0.0657, Recon Loss: 0.0641, VQ Loss: 0.0016, Perplexity: 16.8867\n",
      "Epoch [22/100] Batch [50/63] Loss: 0.0816, Recon Loss: 0.0780, VQ Loss: 0.0036, Perplexity: 21.3167\n",
      "Epoch [22/100] Batch [60/63] Loss: 0.0746, Recon Loss: 0.0723, VQ Loss: 0.0023, Perplexity: 18.9185\n",
      "====> Epoch: 22 Average loss: 0.0706\n",
      "Epoch [23/100] Batch [0/63] Loss: 0.0728, Recon Loss: 0.0680, VQ Loss: 0.0048, Perplexity: 22.2604\n",
      "Epoch [23/100] Batch [10/63] Loss: 0.0702, Recon Loss: 0.0675, VQ Loss: 0.0027, Perplexity: 20.9709\n",
      "Epoch [23/100] Batch [20/63] Loss: 0.0673, Recon Loss: 0.0640, VQ Loss: 0.0033, Perplexity: 18.1165\n",
      "Epoch [23/100] Batch [30/63] Loss: 0.0735, Recon Loss: 0.0710, VQ Loss: 0.0025, Perplexity: 21.3167\n",
      "Epoch [23/100] Batch [40/63] Loss: 0.0734, Recon Loss: 0.0712, VQ Loss: 0.0022, Perplexity: 20.4129\n",
      "Epoch [23/100] Batch [50/63] Loss: 0.0700, Recon Loss: 0.0680, VQ Loss: 0.0020, Perplexity: 21.6681\n",
      "Epoch [23/100] Batch [60/63] Loss: 0.0660, Recon Loss: 0.0640, VQ Loss: 0.0020, Perplexity: 18.6117\n",
      "====> Epoch: 23 Average loss: 0.0703\n",
      "Epoch [24/100] Batch [0/63] Loss: 0.0623, Recon Loss: 0.0599, VQ Loss: 0.0024, Perplexity: 21.3167\n",
      "Epoch [24/100] Batch [10/63] Loss: 0.0692, Recon Loss: 0.0660, VQ Loss: 0.0031, Perplexity: 20.7494\n",
      "Epoch [24/100] Batch [20/63] Loss: 0.0776, Recon Loss: 0.0735, VQ Loss: 0.0042, Perplexity: 22.2604\n",
      "Epoch [24/100] Batch [30/63] Loss: 0.0802, Recon Loss: 0.0777, VQ Loss: 0.0024, Perplexity: 17.9251\n",
      "Epoch [24/100] Batch [40/63] Loss: 0.0739, Recon Loss: 0.0712, VQ Loss: 0.0027, Perplexity: 21.3167\n",
      "Epoch [24/100] Batch [50/63] Loss: 0.0772, Recon Loss: 0.0747, VQ Loss: 0.0025, Perplexity: 17.3484\n",
      "Epoch [24/100] Batch [60/63] Loss: 0.0623, Recon Loss: 0.0582, VQ Loss: 0.0041, Perplexity: 19.7561\n",
      "====> Epoch: 24 Average loss: 0.0701\n",
      "Epoch [25/100] Batch [0/63] Loss: 0.0683, Recon Loss: 0.0667, VQ Loss: 0.0016, Perplexity: 21.6681\n",
      "Epoch [25/100] Batch [10/63] Loss: 0.0637, Recon Loss: 0.0607, VQ Loss: 0.0030, Perplexity: 21.8994\n",
      "Epoch [25/100] Batch [20/63] Loss: 0.0621, Recon Loss: 0.0602, VQ Loss: 0.0018, Perplexity: 23.2460\n",
      "Epoch [25/100] Batch [30/63] Loss: 0.0676, Recon Loss: 0.0627, VQ Loss: 0.0049, Perplexity: 20.9709\n",
      "Epoch [25/100] Batch [40/63] Loss: 0.0811, Recon Loss: 0.0766, VQ Loss: 0.0045, Perplexity: 21.3167\n",
      "Epoch [25/100] Batch [50/63] Loss: 0.0778, Recon Loss: 0.0740, VQ Loss: 0.0039, Perplexity: 22.2604\n",
      "Epoch [25/100] Batch [60/63] Loss: 0.0623, Recon Loss: 0.0608, VQ Loss: 0.0015, Perplexity: 23.2460\n",
      "====> Epoch: 25 Average loss: 0.0696\n",
      "Epoch [26/100] Batch [0/63] Loss: 0.0685, Recon Loss: 0.0656, VQ Loss: 0.0029, Perplexity: 21.3167\n",
      "Epoch [26/100] Batch [10/63] Loss: 0.0714, Recon Loss: 0.0683, VQ Loss: 0.0031, Perplexity: 21.8994\n",
      "Epoch [26/100] Batch [20/63] Loss: 0.0685, Recon Loss: 0.0656, VQ Loss: 0.0029, Perplexity: 25.7678\n",
      "Epoch [26/100] Batch [30/63] Loss: 0.0699, Recon Loss: 0.0672, VQ Loss: 0.0027, Perplexity: 19.7561\n",
      "Epoch [26/100] Batch [40/63] Loss: 0.0673, Recon Loss: 0.0656, VQ Loss: 0.0017, Perplexity: 22.2604\n",
      "Epoch [26/100] Batch [50/63] Loss: 0.0801, Recon Loss: 0.0773, VQ Loss: 0.0028, Perplexity: 16.8503\n",
      "Epoch [26/100] Batch [60/63] Loss: 0.0603, Recon Loss: 0.0553, VQ Loss: 0.0050, Perplexity: 16.6129\n",
      "====> Epoch: 26 Average loss: 0.0693\n",
      "Epoch [27/100] Batch [0/63] Loss: 0.0661, Recon Loss: 0.0613, VQ Loss: 0.0048, Perplexity: 24.2752\n",
      "Epoch [27/100] Batch [10/63] Loss: 0.0592, Recon Loss: 0.0550, VQ Loss: 0.0042, Perplexity: 23.2460\n",
      "Epoch [27/100] Batch [20/63] Loss: 0.0621, Recon Loss: 0.0589, VQ Loss: 0.0032, Perplexity: 20.0385\n",
      "Epoch [27/100] Batch [30/63] Loss: 0.0772, Recon Loss: 0.0715, VQ Loss: 0.0056, Perplexity: 17.1651\n",
      "Epoch [27/100] Batch [40/63] Loss: 0.0686, Recon Loss: 0.0644, VQ Loss: 0.0042, Perplexity: 17.6344\n",
      "Epoch [27/100] Batch [50/63] Loss: 0.0688, Recon Loss: 0.0661, VQ Loss: 0.0028, Perplexity: 22.6274\n",
      "Epoch [27/100] Batch [60/63] Loss: 0.0720, Recon Loss: 0.0691, VQ Loss: 0.0029, Perplexity: 23.6292\n",
      "====> Epoch: 27 Average loss: 0.0693\n",
      "Epoch [28/100] Batch [0/63] Loss: 0.0661, Recon Loss: 0.0637, VQ Loss: 0.0023, Perplexity: 23.6292\n",
      "Epoch [28/100] Batch [10/63] Loss: 0.0608, Recon Loss: 0.0570, VQ Loss: 0.0038, Perplexity: 22.2604\n",
      "Epoch [28/100] Batch [20/63] Loss: 0.0795, Recon Loss: 0.0754, VQ Loss: 0.0042, Perplexity: 17.0670\n",
      "Epoch [28/100] Batch [30/63] Loss: 0.0672, Recon Loss: 0.0590, VQ Loss: 0.0082, Perplexity: 21.3167\n",
      "Epoch [28/100] Batch [40/63] Loss: 0.0514, Recon Loss: 0.0489, VQ Loss: 0.0025, Perplexity: 23.6292\n",
      "Epoch [28/100] Batch [50/63] Loss: 0.0686, Recon Loss: 0.0638, VQ Loss: 0.0048, Perplexity: 23.2460\n",
      "Epoch [28/100] Batch [60/63] Loss: 0.0689, Recon Loss: 0.0664, VQ Loss: 0.0025, Perplexity: 20.9709\n",
      "====> Epoch: 28 Average loss: 0.0692\n",
      "Epoch [29/100] Batch [0/63] Loss: 0.0636, Recon Loss: 0.0595, VQ Loss: 0.0042, Perplexity: 23.2460\n",
      "Epoch [29/100] Batch [10/63] Loss: 0.0745, Recon Loss: 0.0690, VQ Loss: 0.0055, Perplexity: 20.4129\n",
      "Epoch [29/100] Batch [20/63] Loss: 0.0682, Recon Loss: 0.0642, VQ Loss: 0.0040, Perplexity: 20.4129\n",
      "Epoch [29/100] Batch [30/63] Loss: 0.0670, Recon Loss: 0.0626, VQ Loss: 0.0044, Perplexity: 26.4723\n",
      "Epoch [29/100] Batch [40/63] Loss: 0.0644, Recon Loss: 0.0608, VQ Loss: 0.0036, Perplexity: 21.3167\n",
      "Epoch [29/100] Batch [50/63] Loss: 0.0688, Recon Loss: 0.0661, VQ Loss: 0.0027, Perplexity: 23.8815\n",
      "Epoch [29/100] Batch [60/63] Loss: 0.0819, Recon Loss: 0.0759, VQ Loss: 0.0060, Perplexity: 19.7135\n",
      "====> Epoch: 29 Average loss: 0.0694\n",
      "Epoch [30/100] Batch [0/63] Loss: 0.0713, Recon Loss: 0.0682, VQ Loss: 0.0031, Perplexity: 18.4151\n",
      "Epoch [30/100] Batch [10/63] Loss: 0.0601, Recon Loss: 0.0571, VQ Loss: 0.0031, Perplexity: 25.3499\n",
      "Epoch [30/100] Batch [20/63] Loss: 0.0707, Recon Loss: 0.0680, VQ Loss: 0.0027, Perplexity: 23.2460\n",
      "Epoch [30/100] Batch [30/63] Loss: 0.0633, Recon Loss: 0.0610, VQ Loss: 0.0023, Perplexity: 22.2604\n",
      "Epoch [30/100] Batch [40/63] Loss: 0.0669, Recon Loss: 0.0624, VQ Loss: 0.0045, Perplexity: 22.8690\n",
      "Epoch [30/100] Batch [50/63] Loss: 0.0714, Recon Loss: 0.0657, VQ Loss: 0.0057, Perplexity: 19.2304\n",
      "Epoch [30/100] Batch [60/63] Loss: 0.0594, Recon Loss: 0.0563, VQ Loss: 0.0031, Perplexity: 22.6274\n",
      "====> Epoch: 30 Average loss: 0.0698\n",
      "Epoch [31/100] Batch [0/63] Loss: 0.0705, Recon Loss: 0.0658, VQ Loss: 0.0046, Perplexity: 26.9087\n",
      "Epoch [31/100] Batch [10/63] Loss: 0.0626, Recon Loss: 0.0589, VQ Loss: 0.0036, Perplexity: 18.9185\n",
      "Epoch [31/100] Batch [20/63] Loss: 0.0757, Recon Loss: 0.0661, VQ Loss: 0.0097, Perplexity: 22.2604\n",
      "Epoch [31/100] Batch [30/63] Loss: 0.0653, Recon Loss: 0.0618, VQ Loss: 0.0034, Perplexity: 24.6754\n",
      "Epoch [31/100] Batch [40/63] Loss: 0.0692, Recon Loss: 0.0637, VQ Loss: 0.0055, Perplexity: 20.0818\n",
      "Epoch [31/100] Batch [50/63] Loss: 0.0753, Recon Loss: 0.0675, VQ Loss: 0.0077, Perplexity: 22.2604\n",
      "Epoch [31/100] Batch [60/63] Loss: 0.0714, Recon Loss: 0.0682, VQ Loss: 0.0032, Perplexity: 21.3167\n",
      "====> Epoch: 31 Average loss: 0.0706\n",
      "Epoch [32/100] Batch [0/63] Loss: 0.0696, Recon Loss: 0.0610, VQ Loss: 0.0086, Perplexity: 22.2604\n",
      "Epoch [32/100] Batch [10/63] Loss: 0.0752, Recon Loss: 0.0678, VQ Loss: 0.0074, Perplexity: 24.2752\n",
      "Epoch [32/100] Batch [20/63] Loss: 0.0590, Recon Loss: 0.0544, VQ Loss: 0.0047, Perplexity: 19.2304\n",
      "Epoch [32/100] Batch [30/63] Loss: 0.0679, Recon Loss: 0.0601, VQ Loss: 0.0079, Perplexity: 23.2460\n",
      "Epoch [32/100] Batch [40/63] Loss: 0.0753, Recon Loss: 0.0664, VQ Loss: 0.0089, Perplexity: 23.6292\n",
      "Epoch [32/100] Batch [50/63] Loss: 0.0662, Recon Loss: 0.0606, VQ Loss: 0.0055, Perplexity: 21.6681\n",
      "Epoch [32/100] Batch [60/63] Loss: 0.0754, Recon Loss: 0.0653, VQ Loss: 0.0101, Perplexity: 20.4129\n",
      "====> Epoch: 32 Average loss: 0.0720\n",
      "Epoch [33/100] Batch [0/63] Loss: 0.0728, Recon Loss: 0.0649, VQ Loss: 0.0080, Perplexity: 20.9709\n",
      "Epoch [33/100] Batch [10/63] Loss: 0.0780, Recon Loss: 0.0676, VQ Loss: 0.0103, Perplexity: 26.9087\n",
      "Epoch [33/100] Batch [20/63] Loss: 0.0790, Recon Loss: 0.0699, VQ Loss: 0.0091, Perplexity: 20.0818\n",
      "Epoch [33/100] Batch [30/63] Loss: 0.0720, Recon Loss: 0.0620, VQ Loss: 0.0100, Perplexity: 20.0818\n",
      "Epoch [33/100] Batch [40/63] Loss: 0.0814, Recon Loss: 0.0667, VQ Loss: 0.0146, Perplexity: 19.2304\n",
      "Epoch [33/100] Batch [50/63] Loss: 0.0678, Recon Loss: 0.0614, VQ Loss: 0.0064, Perplexity: 22.8690\n",
      "Epoch [33/100] Batch [60/63] Loss: 0.0728, Recon Loss: 0.0616, VQ Loss: 0.0111, Perplexity: 24.6754\n",
      "====> Epoch: 33 Average loss: 0.0733\n",
      "Epoch [34/100] Batch [0/63] Loss: 0.0680, Recon Loss: 0.0580, VQ Loss: 0.0100, Perplexity: 22.2604\n",
      "Epoch [34/100] Batch [10/63] Loss: 0.0763, Recon Loss: 0.0659, VQ Loss: 0.0104, Perplexity: 26.9087\n",
      "Epoch [34/100] Batch [20/63] Loss: 0.0809, Recon Loss: 0.0658, VQ Loss: 0.0151, Perplexity: 21.8994\n",
      "Epoch [34/100] Batch [30/63] Loss: 0.0704, Recon Loss: 0.0562, VQ Loss: 0.0142, Perplexity: 24.6754\n",
      "Epoch [34/100] Batch [40/63] Loss: 0.0738, Recon Loss: 0.0677, VQ Loss: 0.0061, Perplexity: 19.5475\n",
      "Epoch [34/100] Batch [50/63] Loss: 0.0748, Recon Loss: 0.0605, VQ Loss: 0.0143, Perplexity: 19.5475\n",
      "Epoch [34/100] Batch [60/63] Loss: 0.0771, Recon Loss: 0.0646, VQ Loss: 0.0125, Perplexity: 20.0818\n",
      "====> Epoch: 34 Average loss: 0.0741\n",
      "Epoch [35/100] Batch [0/63] Loss: 0.0668, Recon Loss: 0.0530, VQ Loss: 0.0138, Perplexity: 19.7561\n",
      "Epoch [35/100] Batch [10/63] Loss: 0.0686, Recon Loss: 0.0614, VQ Loss: 0.0072, Perplexity: 24.6754\n",
      "Epoch [35/100] Batch [20/63] Loss: 0.0770, Recon Loss: 0.0671, VQ Loss: 0.0100, Perplexity: 21.8994\n",
      "Epoch [35/100] Batch [30/63] Loss: 0.0743, Recon Loss: 0.0678, VQ Loss: 0.0066, Perplexity: 23.2460\n",
      "Epoch [35/100] Batch [40/63] Loss: 0.0797, Recon Loss: 0.0651, VQ Loss: 0.0146, Perplexity: 21.8994\n",
      "Epoch [35/100] Batch [50/63] Loss: 0.0626, Recon Loss: 0.0538, VQ Loss: 0.0088, Perplexity: 21.8994\n",
      "Epoch [35/100] Batch [60/63] Loss: 0.0739, Recon Loss: 0.0611, VQ Loss: 0.0128, Perplexity: 27.6443\n",
      "====> Epoch: 35 Average loss: 0.0731\n",
      "Epoch [36/100] Batch [0/63] Loss: 0.0642, Recon Loss: 0.0545, VQ Loss: 0.0096, Perplexity: 20.4129\n",
      "Epoch [36/100] Batch [10/63] Loss: 0.0699, Recon Loss: 0.0618, VQ Loss: 0.0081, Perplexity: 19.7561\n",
      "Epoch [36/100] Batch [20/63] Loss: 0.0677, Recon Loss: 0.0573, VQ Loss: 0.0104, Perplexity: 18.2206\n",
      "Epoch [36/100] Batch [30/63] Loss: 0.0780, Recon Loss: 0.0638, VQ Loss: 0.0142, Perplexity: 21.6681\n",
      "Epoch [36/100] Batch [40/63] Loss: 0.0767, Recon Loss: 0.0628, VQ Loss: 0.0139, Perplexity: 19.2304\n",
      "Epoch [36/100] Batch [50/63] Loss: 0.0819, Recon Loss: 0.0721, VQ Loss: 0.0098, Perplexity: 21.8994\n",
      "Epoch [36/100] Batch [60/63] Loss: 0.0703, Recon Loss: 0.0611, VQ Loss: 0.0092, Perplexity: 24.2752\n",
      "====> Epoch: 36 Average loss: 0.0725\n",
      "Epoch [37/100] Batch [0/63] Loss: 0.0776, Recon Loss: 0.0637, VQ Loss: 0.0139, Perplexity: 19.7561\n",
      "Epoch [37/100] Batch [10/63] Loss: 0.0758, Recon Loss: 0.0669, VQ Loss: 0.0089, Perplexity: 18.7187\n",
      "Epoch [37/100] Batch [20/63] Loss: 0.0738, Recon Loss: 0.0661, VQ Loss: 0.0077, Perplexity: 23.2460\n",
      "Epoch [37/100] Batch [30/63] Loss: 0.0755, Recon Loss: 0.0656, VQ Loss: 0.0100, Perplexity: 25.3499\n",
      "Epoch [37/100] Batch [40/63] Loss: 0.0709, Recon Loss: 0.0601, VQ Loss: 0.0108, Perplexity: 20.7494\n",
      "Epoch [37/100] Batch [50/63] Loss: 0.0752, Recon Loss: 0.0666, VQ Loss: 0.0086, Perplexity: 22.6274\n",
      "Epoch [37/100] Batch [60/63] Loss: 0.0741, Recon Loss: 0.0649, VQ Loss: 0.0092, Perplexity: 24.2752\n",
      "====> Epoch: 37 Average loss: 0.0711\n",
      "Epoch [38/100] Batch [0/63] Loss: 0.0746, Recon Loss: 0.0635, VQ Loss: 0.0111, Perplexity: 25.7678\n",
      "Epoch [38/100] Batch [10/63] Loss: 0.0780, Recon Loss: 0.0645, VQ Loss: 0.0136, Perplexity: 25.3499\n",
      "Epoch [38/100] Batch [20/63] Loss: 0.0721, Recon Loss: 0.0622, VQ Loss: 0.0100, Perplexity: 19.8697\n",
      "Epoch [38/100] Batch [30/63] Loss: 0.0673, Recon Loss: 0.0591, VQ Loss: 0.0082, Perplexity: 21.3167\n",
      "Epoch [38/100] Batch [40/63] Loss: 0.0767, Recon Loss: 0.0674, VQ Loss: 0.0093, Perplexity: 25.3499\n",
      "Epoch [38/100] Batch [50/63] Loss: 0.0654, Recon Loss: 0.0578, VQ Loss: 0.0076, Perplexity: 23.2460\n",
      "Epoch [38/100] Batch [60/63] Loss: 0.0753, Recon Loss: 0.0657, VQ Loss: 0.0095, Perplexity: 17.4481\n",
      "====> Epoch: 38 Average loss: 0.0707\n",
      "Epoch [39/100] Batch [0/63] Loss: 0.0738, Recon Loss: 0.0670, VQ Loss: 0.0069, Perplexity: 25.7678\n",
      "Epoch [39/100] Batch [10/63] Loss: 0.0654, Recon Loss: 0.0577, VQ Loss: 0.0077, Perplexity: 20.4129\n",
      "Epoch [39/100] Batch [20/63] Loss: 0.0700, Recon Loss: 0.0618, VQ Loss: 0.0082, Perplexity: 25.7678\n",
      "Epoch [39/100] Batch [30/63] Loss: 0.0650, Recon Loss: 0.0588, VQ Loss: 0.0062, Perplexity: 25.3499\n",
      "Epoch [39/100] Batch [40/63] Loss: 0.0727, Recon Loss: 0.0608, VQ Loss: 0.0118, Perplexity: 20.4129\n",
      "Epoch [39/100] Batch [50/63] Loss: 0.0729, Recon Loss: 0.0621, VQ Loss: 0.0108, Perplexity: 22.2604\n",
      "Epoch [39/100] Batch [60/63] Loss: 0.0698, Recon Loss: 0.0611, VQ Loss: 0.0087, Perplexity: 23.2460\n",
      "====> Epoch: 39 Average loss: 0.0702\n",
      "Epoch [40/100] Batch [0/63] Loss: 0.0867, Recon Loss: 0.0763, VQ Loss: 0.0105, Perplexity: 24.6754\n",
      "Epoch [40/100] Batch [10/63] Loss: 0.0682, Recon Loss: 0.0619, VQ Loss: 0.0063, Perplexity: 24.2752\n",
      "Epoch [40/100] Batch [20/63] Loss: 0.0572, Recon Loss: 0.0516, VQ Loss: 0.0056, Perplexity: 23.8815\n",
      "Epoch [40/100] Batch [30/63] Loss: 0.0654, Recon Loss: 0.0596, VQ Loss: 0.0058, Perplexity: 23.6292\n",
      "Epoch [40/100] Batch [40/63] Loss: 0.0682, Recon Loss: 0.0572, VQ Loss: 0.0110, Perplexity: 25.3499\n",
      "Epoch [40/100] Batch [50/63] Loss: 0.0685, Recon Loss: 0.0618, VQ Loss: 0.0067, Perplexity: 22.6274\n",
      "Epoch [40/100] Batch [60/63] Loss: 0.0813, Recon Loss: 0.0724, VQ Loss: 0.0089, Perplexity: 22.2604\n",
      "====> Epoch: 40 Average loss: 0.0698\n",
      "Epoch [41/100] Batch [0/63] Loss: 0.0715, Recon Loss: 0.0630, VQ Loss: 0.0085, Perplexity: 25.3499\n",
      "Epoch [41/100] Batch [10/63] Loss: 0.0743, Recon Loss: 0.0642, VQ Loss: 0.0101, Perplexity: 22.8690\n",
      "Epoch [41/100] Batch [20/63] Loss: 0.0676, Recon Loss: 0.0616, VQ Loss: 0.0060, Perplexity: 26.9087\n",
      "Epoch [41/100] Batch [30/63] Loss: 0.0702, Recon Loss: 0.0625, VQ Loss: 0.0076, Perplexity: 18.1165\n",
      "Epoch [41/100] Batch [40/63] Loss: 0.0675, Recon Loss: 0.0604, VQ Loss: 0.0071, Perplexity: 21.3167\n",
      "Epoch [41/100] Batch [50/63] Loss: 0.0666, Recon Loss: 0.0580, VQ Loss: 0.0086, Perplexity: 19.7561\n",
      "Epoch [41/100] Batch [60/63] Loss: 0.0751, Recon Loss: 0.0686, VQ Loss: 0.0065, Perplexity: 25.7678\n",
      "====> Epoch: 41 Average loss: 0.0694\n",
      "Epoch [42/100] Batch [0/63] Loss: 0.0666, Recon Loss: 0.0599, VQ Loss: 0.0067, Perplexity: 25.7678\n",
      "Epoch [42/100] Batch [10/63] Loss: 0.0697, Recon Loss: 0.0608, VQ Loss: 0.0089, Perplexity: 22.8690\n",
      "Epoch [42/100] Batch [20/63] Loss: 0.0768, Recon Loss: 0.0635, VQ Loss: 0.0132, Perplexity: 25.3499\n",
      "Epoch [42/100] Batch [30/63] Loss: 0.0616, Recon Loss: 0.0539, VQ Loss: 0.0077, Perplexity: 20.4129\n",
      "Epoch [42/100] Batch [40/63] Loss: 0.0716, Recon Loss: 0.0639, VQ Loss: 0.0076, Perplexity: 25.7678\n",
      "Epoch [42/100] Batch [50/63] Loss: 0.0649, Recon Loss: 0.0572, VQ Loss: 0.0077, Perplexity: 24.2752\n",
      "Epoch [42/100] Batch [60/63] Loss: 0.0929, Recon Loss: 0.0783, VQ Loss: 0.0146, Perplexity: 23.8815\n",
      "====> Epoch: 42 Average loss: 0.0692\n",
      "Epoch [43/100] Batch [0/63] Loss: 0.0692, Recon Loss: 0.0620, VQ Loss: 0.0072, Perplexity: 25.7678\n",
      "Epoch [43/100] Batch [10/63] Loss: 0.0703, Recon Loss: 0.0590, VQ Loss: 0.0113, Perplexity: 25.3499\n",
      "Epoch [43/100] Batch [20/63] Loss: 0.0662, Recon Loss: 0.0590, VQ Loss: 0.0072, Perplexity: 27.6443\n",
      "Epoch [43/100] Batch [30/63] Loss: 0.0723, Recon Loss: 0.0631, VQ Loss: 0.0092, Perplexity: 22.2604\n",
      "Epoch [43/100] Batch [40/63] Loss: 0.0660, Recon Loss: 0.0593, VQ Loss: 0.0067, Perplexity: 16.1708\n",
      "Epoch [43/100] Batch [50/63] Loss: 0.0582, Recon Loss: 0.0488, VQ Loss: 0.0093, Perplexity: 19.7561\n",
      "Epoch [43/100] Batch [60/63] Loss: 0.0748, Recon Loss: 0.0628, VQ Loss: 0.0120, Perplexity: 21.3167\n",
      "====> Epoch: 43 Average loss: 0.0693\n",
      "Epoch [44/100] Batch [0/63] Loss: 0.0684, Recon Loss: 0.0607, VQ Loss: 0.0077, Perplexity: 25.7678\n",
      "Epoch [44/100] Batch [10/63] Loss: 0.0725, Recon Loss: 0.0637, VQ Loss: 0.0088, Perplexity: 23.2460\n",
      "Epoch [44/100] Batch [20/63] Loss: 0.0607, Recon Loss: 0.0545, VQ Loss: 0.0063, Perplexity: 25.7678\n",
      "Epoch [44/100] Batch [30/63] Loss: 0.0681, Recon Loss: 0.0605, VQ Loss: 0.0076, Perplexity: 25.3499\n",
      "Epoch [44/100] Batch [40/63] Loss: 0.0611, Recon Loss: 0.0541, VQ Loss: 0.0070, Perplexity: 25.3499\n",
      "Epoch [44/100] Batch [50/63] Loss: 0.0687, Recon Loss: 0.0608, VQ Loss: 0.0079, Perplexity: 23.6292\n",
      "Epoch [44/100] Batch [60/63] Loss: 0.0619, Recon Loss: 0.0562, VQ Loss: 0.0057, Perplexity: 28.1000\n",
      "====> Epoch: 44 Average loss: 0.0691\n",
      "Epoch [45/100] Batch [0/63] Loss: 0.0679, Recon Loss: 0.0594, VQ Loss: 0.0085, Perplexity: 24.6754\n",
      "Epoch [45/100] Batch [10/63] Loss: 0.0706, Recon Loss: 0.0587, VQ Loss: 0.0118, Perplexity: 22.2604\n",
      "Epoch [45/100] Batch [20/63] Loss: 0.0656, Recon Loss: 0.0588, VQ Loss: 0.0067, Perplexity: 21.8994\n",
      "Epoch [45/100] Batch [30/63] Loss: 0.0671, Recon Loss: 0.0598, VQ Loss: 0.0072, Perplexity: 23.6292\n",
      "Epoch [45/100] Batch [40/63] Loss: 0.0757, Recon Loss: 0.0664, VQ Loss: 0.0093, Perplexity: 23.2460\n",
      "Epoch [45/100] Batch [50/63] Loss: 0.0663, Recon Loss: 0.0596, VQ Loss: 0.0066, Perplexity: 21.3167\n",
      "Epoch [45/100] Batch [60/63] Loss: 0.0763, Recon Loss: 0.0668, VQ Loss: 0.0095, Perplexity: 26.9087\n",
      "====> Epoch: 45 Average loss: 0.0689\n",
      "Epoch [46/100] Batch [0/63] Loss: 0.0646, Recon Loss: 0.0573, VQ Loss: 0.0073, Perplexity: 23.2460\n",
      "Epoch [46/100] Batch [10/63] Loss: 0.0780, Recon Loss: 0.0679, VQ Loss: 0.0101, Perplexity: 21.8994\n",
      "Epoch [46/100] Batch [20/63] Loss: 0.0621, Recon Loss: 0.0522, VQ Loss: 0.0099, Perplexity: 22.8690\n",
      "Epoch [46/100] Batch [30/63] Loss: 0.0770, Recon Loss: 0.0652, VQ Loss: 0.0118, Perplexity: 24.2752\n",
      "Epoch [46/100] Batch [40/63] Loss: 0.0779, Recon Loss: 0.0670, VQ Loss: 0.0110, Perplexity: 22.6274\n",
      "Epoch [46/100] Batch [50/63] Loss: 0.0665, Recon Loss: 0.0593, VQ Loss: 0.0072, Perplexity: 24.2752\n",
      "Epoch [46/100] Batch [60/63] Loss: 0.0645, Recon Loss: 0.0556, VQ Loss: 0.0089, Perplexity: 25.7678\n",
      "====> Epoch: 46 Average loss: 0.0689\n",
      "Epoch [47/100] Batch [0/63] Loss: 0.0663, Recon Loss: 0.0577, VQ Loss: 0.0086, Perplexity: 26.4723\n",
      "Epoch [47/100] Batch [10/63] Loss: 0.0575, Recon Loss: 0.0513, VQ Loss: 0.0062, Perplexity: 23.6292\n",
      "Epoch [47/100] Batch [20/63] Loss: 0.0606, Recon Loss: 0.0545, VQ Loss: 0.0062, Perplexity: 22.6274\n",
      "Epoch [47/100] Batch [30/63] Loss: 0.0664, Recon Loss: 0.0605, VQ Loss: 0.0059, Perplexity: 22.2604\n",
      "Epoch [47/100] Batch [40/63] Loss: 0.0683, Recon Loss: 0.0624, VQ Loss: 0.0058, Perplexity: 19.7561\n",
      "Epoch [47/100] Batch [50/63] Loss: 0.0634, Recon Loss: 0.0571, VQ Loss: 0.0062, Perplexity: 25.3499\n",
      "Epoch [47/100] Batch [60/63] Loss: 0.0784, Recon Loss: 0.0704, VQ Loss: 0.0081, Perplexity: 25.7678\n",
      "====> Epoch: 47 Average loss: 0.0688\n",
      "Epoch [48/100] Batch [0/63] Loss: 0.0874, Recon Loss: 0.0755, VQ Loss: 0.0119, Perplexity: 23.8815\n",
      "Epoch [48/100] Batch [10/63] Loss: 0.0654, Recon Loss: 0.0594, VQ Loss: 0.0060, Perplexity: 23.2460\n",
      "Epoch [48/100] Batch [20/63] Loss: 0.0743, Recon Loss: 0.0685, VQ Loss: 0.0058, Perplexity: 16.6129\n",
      "Epoch [48/100] Batch [30/63] Loss: 0.0711, Recon Loss: 0.0595, VQ Loss: 0.0116, Perplexity: 20.9709\n",
      "Epoch [48/100] Batch [40/63] Loss: 0.0661, Recon Loss: 0.0586, VQ Loss: 0.0075, Perplexity: 23.2460\n",
      "Epoch [48/100] Batch [50/63] Loss: 0.0669, Recon Loss: 0.0615, VQ Loss: 0.0054, Perplexity: 26.9087\n",
      "Epoch [48/100] Batch [60/63] Loss: 0.0755, Recon Loss: 0.0689, VQ Loss: 0.0065, Perplexity: 24.2752\n",
      "====> Epoch: 48 Average loss: 0.0687\n",
      "Epoch [49/100] Batch [0/63] Loss: 0.0748, Recon Loss: 0.0629, VQ Loss: 0.0119, Perplexity: 26.9087\n",
      "Epoch [49/100] Batch [10/63] Loss: 0.0665, Recon Loss: 0.0591, VQ Loss: 0.0073, Perplexity: 22.8690\n",
      "Epoch [49/100] Batch [20/63] Loss: 0.0710, Recon Loss: 0.0598, VQ Loss: 0.0112, Perplexity: 23.6292\n",
      "Epoch [49/100] Batch [30/63] Loss: 0.0831, Recon Loss: 0.0722, VQ Loss: 0.0109, Perplexity: 24.6754\n",
      "Epoch [49/100] Batch [40/63] Loss: 0.0846, Recon Loss: 0.0761, VQ Loss: 0.0085, Perplexity: 22.2604\n",
      "Epoch [49/100] Batch [50/63] Loss: 0.0713, Recon Loss: 0.0627, VQ Loss: 0.0087, Perplexity: 21.8994\n",
      "Epoch [49/100] Batch [60/63] Loss: 0.0708, Recon Loss: 0.0628, VQ Loss: 0.0080, Perplexity: 29.3441\n",
      "====> Epoch: 49 Average loss: 0.0686\n",
      "Epoch [50/100] Batch [0/63] Loss: 0.0755, Recon Loss: 0.0679, VQ Loss: 0.0076, Perplexity: 25.3499\n",
      "Epoch [50/100] Batch [10/63] Loss: 0.0696, Recon Loss: 0.0603, VQ Loss: 0.0093, Perplexity: 21.8994\n",
      "Epoch [50/100] Batch [20/63] Loss: 0.0630, Recon Loss: 0.0558, VQ Loss: 0.0072, Perplexity: 20.7494\n",
      "Epoch [50/100] Batch [30/63] Loss: 0.0789, Recon Loss: 0.0653, VQ Loss: 0.0136, Perplexity: 21.3167\n",
      "Epoch [50/100] Batch [40/63] Loss: 0.0656, Recon Loss: 0.0575, VQ Loss: 0.0082, Perplexity: 24.6754\n",
      "Epoch [50/100] Batch [50/63] Loss: 0.0679, Recon Loss: 0.0610, VQ Loss: 0.0069, Perplexity: 28.1000\n",
      "Epoch [50/100] Batch [60/63] Loss: 0.0621, Recon Loss: 0.0556, VQ Loss: 0.0065, Perplexity: 28.1000\n",
      "====> Epoch: 50 Average loss: 0.0684\n",
      "Epoch [51/100] Batch [0/63] Loss: 0.0618, Recon Loss: 0.0544, VQ Loss: 0.0073, Perplexity: 26.9087\n",
      "Epoch [51/100] Batch [10/63] Loss: 0.0657, Recon Loss: 0.0586, VQ Loss: 0.0071, Perplexity: 25.7678\n",
      "Epoch [51/100] Batch [20/63] Loss: 0.0667, Recon Loss: 0.0593, VQ Loss: 0.0074, Perplexity: 22.8690\n",
      "Epoch [51/100] Batch [30/63] Loss: 0.0664, Recon Loss: 0.0563, VQ Loss: 0.0101, Perplexity: 23.6292\n",
      "Epoch [51/100] Batch [40/63] Loss: 0.0622, Recon Loss: 0.0526, VQ Loss: 0.0096, Perplexity: 25.3499\n",
      "Epoch [51/100] Batch [50/63] Loss: 0.0635, Recon Loss: 0.0590, VQ Loss: 0.0045, Perplexity: 22.8690\n",
      "Epoch [51/100] Batch [60/63] Loss: 0.0643, Recon Loss: 0.0553, VQ Loss: 0.0090, Perplexity: 22.8690\n",
      "====> Epoch: 51 Average loss: 0.0682\n",
      "Epoch [52/100] Batch [0/63] Loss: 0.0687, Recon Loss: 0.0607, VQ Loss: 0.0080, Perplexity: 24.6754\n",
      "Epoch [52/100] Batch [10/63] Loss: 0.0738, Recon Loss: 0.0633, VQ Loss: 0.0105, Perplexity: 19.5475\n",
      "Epoch [52/100] Batch [20/63] Loss: 0.0574, Recon Loss: 0.0499, VQ Loss: 0.0075, Perplexity: 28.1000\n",
      "Epoch [52/100] Batch [30/63] Loss: 0.0773, Recon Loss: 0.0722, VQ Loss: 0.0051, Perplexity: 24.6754\n",
      "Epoch [52/100] Batch [40/63] Loss: 0.0677, Recon Loss: 0.0585, VQ Loss: 0.0093, Perplexity: 26.9087\n",
      "Epoch [52/100] Batch [50/63] Loss: 0.0650, Recon Loss: 0.0597, VQ Loss: 0.0053, Perplexity: 17.9251\n",
      "Epoch [52/100] Batch [60/63] Loss: 0.0681, Recon Loss: 0.0560, VQ Loss: 0.0121, Perplexity: 25.3499\n",
      "====> Epoch: 52 Average loss: 0.0684\n",
      "Epoch [53/100] Batch [0/63] Loss: 0.0698, Recon Loss: 0.0631, VQ Loss: 0.0068, Perplexity: 26.4723\n",
      "Epoch [53/100] Batch [10/63] Loss: 0.0641, Recon Loss: 0.0538, VQ Loss: 0.0103, Perplexity: 24.2752\n",
      "Epoch [53/100] Batch [20/63] Loss: 0.0667, Recon Loss: 0.0592, VQ Loss: 0.0075, Perplexity: 25.3499\n",
      "Epoch [53/100] Batch [30/63] Loss: 0.0683, Recon Loss: 0.0569, VQ Loss: 0.0114, Perplexity: 22.2604\n",
      "Epoch [53/100] Batch [40/63] Loss: 0.0727, Recon Loss: 0.0679, VQ Loss: 0.0047, Perplexity: 24.2752\n",
      "Epoch [53/100] Batch [50/63] Loss: 0.0739, Recon Loss: 0.0641, VQ Loss: 0.0099, Perplexity: 23.2460\n",
      "Epoch [53/100] Batch [60/63] Loss: 0.0671, Recon Loss: 0.0598, VQ Loss: 0.0073, Perplexity: 28.1000\n",
      "====> Epoch: 53 Average loss: 0.0684\n",
      "Epoch [54/100] Batch [0/63] Loss: 0.0764, Recon Loss: 0.0693, VQ Loss: 0.0072, Perplexity: 25.7678\n",
      "Epoch [54/100] Batch [10/63] Loss: 0.0662, Recon Loss: 0.0572, VQ Loss: 0.0089, Perplexity: 25.3499\n",
      "Epoch [54/100] Batch [20/63] Loss: 0.0579, Recon Loss: 0.0529, VQ Loss: 0.0050, Perplexity: 25.7678\n",
      "Epoch [54/100] Batch [30/63] Loss: 0.0601, Recon Loss: 0.0538, VQ Loss: 0.0063, Perplexity: 26.0429\n",
      "Epoch [54/100] Batch [40/63] Loss: 0.0579, Recon Loss: 0.0508, VQ Loss: 0.0071, Perplexity: 23.2460\n",
      "Epoch [54/100] Batch [50/63] Loss: 0.0775, Recon Loss: 0.0689, VQ Loss: 0.0085, Perplexity: 23.8815\n",
      "Epoch [54/100] Batch [60/63] Loss: 0.0756, Recon Loss: 0.0656, VQ Loss: 0.0099, Perplexity: 20.6308\n",
      "====> Epoch: 54 Average loss: 0.0681\n",
      "Epoch [55/100] Batch [0/63] Loss: 0.0608, Recon Loss: 0.0519, VQ Loss: 0.0088, Perplexity: 26.9087\n",
      "Epoch [55/100] Batch [10/63] Loss: 0.0719, Recon Loss: 0.0661, VQ Loss: 0.0058, Perplexity: 26.9087\n",
      "Epoch [55/100] Batch [20/63] Loss: 0.0642, Recon Loss: 0.0555, VQ Loss: 0.0087, Perplexity: 24.2752\n",
      "Epoch [55/100] Batch [30/63] Loss: 0.0558, Recon Loss: 0.0491, VQ Loss: 0.0067, Perplexity: 24.2752\n",
      "Epoch [55/100] Batch [40/63] Loss: 0.0685, Recon Loss: 0.0605, VQ Loss: 0.0080, Perplexity: 24.2752\n",
      "Epoch [55/100] Batch [50/63] Loss: 0.0664, Recon Loss: 0.0589, VQ Loss: 0.0075, Perplexity: 28.1000\n",
      "Epoch [55/100] Batch [60/63] Loss: 0.0694, Recon Loss: 0.0611, VQ Loss: 0.0084, Perplexity: 24.2752\n",
      "====> Epoch: 55 Average loss: 0.0681\n",
      "Epoch [56/100] Batch [0/63] Loss: 0.0682, Recon Loss: 0.0607, VQ Loss: 0.0076, Perplexity: 24.9388\n",
      "Epoch [56/100] Batch [10/63] Loss: 0.0804, Recon Loss: 0.0705, VQ Loss: 0.0099, Perplexity: 23.6292\n",
      "Epoch [56/100] Batch [20/63] Loss: 0.0612, Recon Loss: 0.0561, VQ Loss: 0.0051, Perplexity: 22.8690\n",
      "Epoch [56/100] Batch [30/63] Loss: 0.0735, Recon Loss: 0.0642, VQ Loss: 0.0093, Perplexity: 24.2752\n",
      "Epoch [56/100] Batch [40/63] Loss: 0.0672, Recon Loss: 0.0580, VQ Loss: 0.0092, Perplexity: 28.1000\n",
      "Epoch [56/100] Batch [50/63] Loss: 0.0705, Recon Loss: 0.0608, VQ Loss: 0.0098, Perplexity: 24.2752\n",
      "Epoch [56/100] Batch [60/63] Loss: 0.0618, Recon Loss: 0.0545, VQ Loss: 0.0074, Perplexity: 29.3441\n",
      "====> Epoch: 56 Average loss: 0.0683\n",
      "Epoch [57/100] Batch [0/63] Loss: 0.0572, Recon Loss: 0.0521, VQ Loss: 0.0051, Perplexity: 21.8994\n",
      "Epoch [57/100] Batch [10/63] Loss: 0.0729, Recon Loss: 0.0649, VQ Loss: 0.0080, Perplexity: 28.1000\n",
      "Epoch [57/100] Batch [20/63] Loss: 0.0583, Recon Loss: 0.0497, VQ Loss: 0.0086, Perplexity: 29.3441\n",
      "Epoch [57/100] Batch [30/63] Loss: 0.0732, Recon Loss: 0.0631, VQ Loss: 0.0101, Perplexity: 25.7678\n",
      "Epoch [57/100] Batch [40/63] Loss: 0.0696, Recon Loss: 0.0605, VQ Loss: 0.0091, Perplexity: 24.2752\n",
      "Epoch [57/100] Batch [50/63] Loss: 0.0789, Recon Loss: 0.0689, VQ Loss: 0.0100, Perplexity: 29.3441\n",
      "Epoch [57/100] Batch [60/63] Loss: 0.0717, Recon Loss: 0.0616, VQ Loss: 0.0101, Perplexity: 18.7187\n",
      "====> Epoch: 57 Average loss: 0.0681\n",
      "Epoch [58/100] Batch [0/63] Loss: 0.0633, Recon Loss: 0.0548, VQ Loss: 0.0085, Perplexity: 24.2752\n",
      "Epoch [58/100] Batch [10/63] Loss: 0.0709, Recon Loss: 0.0607, VQ Loss: 0.0102, Perplexity: 25.3499\n",
      "Epoch [58/100] Batch [20/63] Loss: 0.0574, Recon Loss: 0.0519, VQ Loss: 0.0054, Perplexity: 26.4723\n",
      "Epoch [58/100] Batch [30/63] Loss: 0.0631, Recon Loss: 0.0528, VQ Loss: 0.0102, Perplexity: 28.1000\n",
      "Epoch [58/100] Batch [40/63] Loss: 0.0752, Recon Loss: 0.0668, VQ Loss: 0.0084, Perplexity: 26.9087\n",
      "Epoch [58/100] Batch [50/63] Loss: 0.0639, Recon Loss: 0.0580, VQ Loss: 0.0059, Perplexity: 20.9709\n",
      "Epoch [58/100] Batch [60/63] Loss: 0.0565, Recon Loss: 0.0503, VQ Loss: 0.0062, Perplexity: 22.8690\n",
      "====> Epoch: 58 Average loss: 0.0679\n",
      "Epoch [59/100] Batch [0/63] Loss: 0.0677, Recon Loss: 0.0612, VQ Loss: 0.0065, Perplexity: 22.8690\n",
      "Epoch [59/100] Batch [10/63] Loss: 0.0637, Recon Loss: 0.0564, VQ Loss: 0.0073, Perplexity: 23.2460\n",
      "Epoch [59/100] Batch [20/63] Loss: 0.0637, Recon Loss: 0.0578, VQ Loss: 0.0059, Perplexity: 25.3499\n",
      "Epoch [59/100] Batch [30/63] Loss: 0.0693, Recon Loss: 0.0597, VQ Loss: 0.0096, Perplexity: 24.2752\n",
      "Epoch [59/100] Batch [40/63] Loss: 0.0663, Recon Loss: 0.0591, VQ Loss: 0.0072, Perplexity: 20.6308\n",
      "Epoch [59/100] Batch [50/63] Loss: 0.0552, Recon Loss: 0.0502, VQ Loss: 0.0050, Perplexity: 29.3441\n",
      "Epoch [59/100] Batch [60/63] Loss: 0.0768, Recon Loss: 0.0659, VQ Loss: 0.0109, Perplexity: 26.9087\n",
      "====> Epoch: 59 Average loss: 0.0677\n",
      "Epoch [60/100] Batch [0/63] Loss: 0.0714, Recon Loss: 0.0599, VQ Loss: 0.0115, Perplexity: 26.9087\n",
      "Epoch [60/100] Batch [10/63] Loss: 0.0700, Recon Loss: 0.0618, VQ Loss: 0.0082, Perplexity: 22.8690\n",
      "Epoch [60/100] Batch [20/63] Loss: 0.0698, Recon Loss: 0.0618, VQ Loss: 0.0080, Perplexity: 24.2752\n",
      "Epoch [60/100] Batch [30/63] Loss: 0.0742, Recon Loss: 0.0639, VQ Loss: 0.0103, Perplexity: 27.6443\n",
      "Epoch [60/100] Batch [40/63] Loss: 0.0691, Recon Loss: 0.0616, VQ Loss: 0.0075, Perplexity: 23.6292\n",
      "Epoch [60/100] Batch [50/63] Loss: 0.0686, Recon Loss: 0.0634, VQ Loss: 0.0053, Perplexity: 23.6292\n",
      "Epoch [60/100] Batch [60/63] Loss: 0.0553, Recon Loss: 0.0488, VQ Loss: 0.0065, Perplexity: 24.2752\n",
      "====> Epoch: 60 Average loss: 0.0680\n",
      "Epoch [61/100] Batch [0/63] Loss: 0.0714, Recon Loss: 0.0631, VQ Loss: 0.0083, Perplexity: 26.9087\n",
      "Epoch [61/100] Batch [10/63] Loss: 0.0632, Recon Loss: 0.0541, VQ Loss: 0.0091, Perplexity: 29.3441\n",
      "Epoch [61/100] Batch [20/63] Loss: 0.0741, Recon Loss: 0.0669, VQ Loss: 0.0072, Perplexity: 23.2460\n",
      "Epoch [61/100] Batch [30/63] Loss: 0.0617, Recon Loss: 0.0538, VQ Loss: 0.0079, Perplexity: 23.6292\n",
      "Epoch [61/100] Batch [40/63] Loss: 0.0612, Recon Loss: 0.0547, VQ Loss: 0.0065, Perplexity: 24.6754\n",
      "Epoch [61/100] Batch [50/63] Loss: 0.0733, Recon Loss: 0.0670, VQ Loss: 0.0063, Perplexity: 26.9087\n",
      "Epoch [61/100] Batch [60/63] Loss: 0.0558, Recon Loss: 0.0503, VQ Loss: 0.0055, Perplexity: 28.1000\n",
      "====> Epoch: 61 Average loss: 0.0676\n",
      "Epoch [62/100] Batch [0/63] Loss: 0.0674, Recon Loss: 0.0592, VQ Loss: 0.0082, Perplexity: 25.7678\n",
      "Epoch [62/100] Batch [10/63] Loss: 0.0599, Recon Loss: 0.0518, VQ Loss: 0.0082, Perplexity: 21.6681\n",
      "Epoch [62/100] Batch [20/63] Loss: 0.0690, Recon Loss: 0.0607, VQ Loss: 0.0083, Perplexity: 25.7678\n",
      "Epoch [62/100] Batch [30/63] Loss: 0.0623, Recon Loss: 0.0527, VQ Loss: 0.0096, Perplexity: 26.4723\n",
      "Epoch [62/100] Batch [40/63] Loss: 0.0682, Recon Loss: 0.0571, VQ Loss: 0.0111, Perplexity: 24.6754\n",
      "Epoch [62/100] Batch [50/63] Loss: 0.0653, Recon Loss: 0.0556, VQ Loss: 0.0097, Perplexity: 25.7678\n",
      "Epoch [62/100] Batch [60/63] Loss: 0.0654, Recon Loss: 0.0601, VQ Loss: 0.0053, Perplexity: 24.2752\n",
      "====> Epoch: 62 Average loss: 0.0678\n",
      "Epoch [63/100] Batch [0/63] Loss: 0.0599, Recon Loss: 0.0548, VQ Loss: 0.0051, Perplexity: 26.4723\n",
      "Epoch [63/100] Batch [10/63] Loss: 0.0723, Recon Loss: 0.0625, VQ Loss: 0.0098, Perplexity: 23.2460\n",
      "Epoch [63/100] Batch [20/63] Loss: 0.0668, Recon Loss: 0.0578, VQ Loss: 0.0090, Perplexity: 23.2460\n",
      "Epoch [63/100] Batch [30/63] Loss: 0.0727, Recon Loss: 0.0664, VQ Loss: 0.0064, Perplexity: 27.6443\n",
      "Epoch [63/100] Batch [40/63] Loss: 0.0604, Recon Loss: 0.0514, VQ Loss: 0.0090, Perplexity: 25.3499\n",
      "Epoch [63/100] Batch [50/63] Loss: 0.0651, Recon Loss: 0.0575, VQ Loss: 0.0076, Perplexity: 26.9087\n",
      "Epoch [63/100] Batch [60/63] Loss: 0.0633, Recon Loss: 0.0547, VQ Loss: 0.0086, Perplexity: 25.3499\n",
      "====> Epoch: 63 Average loss: 0.0675\n",
      "Epoch [64/100] Batch [0/63] Loss: 0.0644, Recon Loss: 0.0553, VQ Loss: 0.0092, Perplexity: 26.9087\n",
      "Epoch [64/100] Batch [10/63] Loss: 0.0722, Recon Loss: 0.0651, VQ Loss: 0.0071, Perplexity: 28.1000\n",
      "Epoch [64/100] Batch [20/63] Loss: 0.0687, Recon Loss: 0.0621, VQ Loss: 0.0066, Perplexity: 24.6754\n",
      "Epoch [64/100] Batch [30/63] Loss: 0.0641, Recon Loss: 0.0570, VQ Loss: 0.0071, Perplexity: 25.7678\n",
      "Epoch [64/100] Batch [40/63] Loss: 0.0669, Recon Loss: 0.0591, VQ Loss: 0.0078, Perplexity: 29.3441\n",
      "Epoch [64/100] Batch [50/63] Loss: 0.0620, Recon Loss: 0.0511, VQ Loss: 0.0109, Perplexity: 24.2752\n",
      "Epoch [64/100] Batch [60/63] Loss: 0.0720, Recon Loss: 0.0621, VQ Loss: 0.0099, Perplexity: 24.6754\n",
      "====> Epoch: 64 Average loss: 0.0675\n",
      "Epoch [65/100] Batch [0/63] Loss: 0.0693, Recon Loss: 0.0567, VQ Loss: 0.0126, Perplexity: 22.6274\n",
      "Epoch [65/100] Batch [10/63] Loss: 0.0643, Recon Loss: 0.0549, VQ Loss: 0.0094, Perplexity: 25.3499\n",
      "Epoch [65/100] Batch [20/63] Loss: 0.0635, Recon Loss: 0.0564, VQ Loss: 0.0071, Perplexity: 29.3441\n",
      "Epoch [65/100] Batch [30/63] Loss: 0.0716, Recon Loss: 0.0597, VQ Loss: 0.0119, Perplexity: 28.1000\n",
      "Epoch [65/100] Batch [40/63] Loss: 0.0745, Recon Loss: 0.0630, VQ Loss: 0.0115, Perplexity: 25.7679\n",
      "Epoch [65/100] Batch [50/63] Loss: 0.0648, Recon Loss: 0.0580, VQ Loss: 0.0068, Perplexity: 26.9087\n",
      "Epoch [65/100] Batch [60/63] Loss: 0.0729, Recon Loss: 0.0623, VQ Loss: 0.0106, Perplexity: 22.8690\n",
      "====> Epoch: 65 Average loss: 0.0676\n",
      "Epoch [66/100] Batch [0/63] Loss: 0.0687, Recon Loss: 0.0618, VQ Loss: 0.0069, Perplexity: 21.8994\n",
      "Epoch [66/100] Batch [10/63] Loss: 0.0637, Recon Loss: 0.0564, VQ Loss: 0.0073, Perplexity: 26.9087\n",
      "Epoch [66/100] Batch [20/63] Loss: 0.0769, Recon Loss: 0.0669, VQ Loss: 0.0100, Perplexity: 27.6443\n",
      "Epoch [66/100] Batch [30/63] Loss: 0.0655, Recon Loss: 0.0575, VQ Loss: 0.0080, Perplexity: 26.9087\n",
      "Epoch [66/100] Batch [40/63] Loss: 0.0707, Recon Loss: 0.0631, VQ Loss: 0.0076, Perplexity: 28.1000\n",
      "Epoch [66/100] Batch [50/63] Loss: 0.0871, Recon Loss: 0.0782, VQ Loss: 0.0089, Perplexity: 30.6433\n",
      "Epoch [66/100] Batch [60/63] Loss: 0.0637, Recon Loss: 0.0542, VQ Loss: 0.0095, Perplexity: 26.9087\n",
      "====> Epoch: 66 Average loss: 0.0674\n",
      "Epoch [67/100] Batch [0/63] Loss: 0.0674, Recon Loss: 0.0580, VQ Loss: 0.0094, Perplexity: 24.2752\n",
      "Epoch [67/100] Batch [10/63] Loss: 0.0638, Recon Loss: 0.0566, VQ Loss: 0.0072, Perplexity: 21.6681\n",
      "Epoch [67/100] Batch [20/63] Loss: 0.0641, Recon Loss: 0.0576, VQ Loss: 0.0065, Perplexity: 21.3167\n",
      "Epoch [67/100] Batch [30/63] Loss: 0.0730, Recon Loss: 0.0655, VQ Loss: 0.0075, Perplexity: 28.1000\n",
      "Epoch [67/100] Batch [40/63] Loss: 0.0622, Recon Loss: 0.0552, VQ Loss: 0.0070, Perplexity: 28.1000\n",
      "Epoch [67/100] Batch [50/63] Loss: 0.0635, Recon Loss: 0.0568, VQ Loss: 0.0066, Perplexity: 27.6443\n",
      "Epoch [67/100] Batch [60/63] Loss: 0.0541, Recon Loss: 0.0494, VQ Loss: 0.0047, Perplexity: 25.7678\n",
      "====> Epoch: 67 Average loss: 0.0674\n",
      "Epoch [68/100] Batch [0/63] Loss: 0.0784, Recon Loss: 0.0683, VQ Loss: 0.0101, Perplexity: 19.8697\n",
      "Epoch [68/100] Batch [10/63] Loss: 0.0578, Recon Loss: 0.0514, VQ Loss: 0.0064, Perplexity: 22.8690\n",
      "Epoch [68/100] Batch [20/63] Loss: 0.0693, Recon Loss: 0.0628, VQ Loss: 0.0065, Perplexity: 23.2460\n",
      "Epoch [68/100] Batch [30/63] Loss: 0.0693, Recon Loss: 0.0592, VQ Loss: 0.0101, Perplexity: 20.9709\n",
      "Epoch [68/100] Batch [40/63] Loss: 0.0693, Recon Loss: 0.0623, VQ Loss: 0.0070, Perplexity: 26.9087\n",
      "Epoch [68/100] Batch [50/63] Loss: 0.0644, Recon Loss: 0.0527, VQ Loss: 0.0116, Perplexity: 28.1000\n",
      "Epoch [68/100] Batch [60/63] Loss: 0.0715, Recon Loss: 0.0600, VQ Loss: 0.0116, Perplexity: 24.2752\n",
      "====> Epoch: 68 Average loss: 0.0673\n",
      "Epoch [69/100] Batch [0/63] Loss: 0.0671, Recon Loss: 0.0576, VQ Loss: 0.0096, Perplexity: 28.1000\n",
      "Epoch [69/100] Batch [10/63] Loss: 0.0698, Recon Loss: 0.0614, VQ Loss: 0.0085, Perplexity: 19.2304\n",
      "Epoch [69/100] Batch [20/63] Loss: 0.0748, Recon Loss: 0.0647, VQ Loss: 0.0101, Perplexity: 25.3499\n",
      "Epoch [69/100] Batch [30/63] Loss: 0.0635, Recon Loss: 0.0563, VQ Loss: 0.0072, Perplexity: 25.3499\n",
      "Epoch [69/100] Batch [40/63] Loss: 0.0772, Recon Loss: 0.0641, VQ Loss: 0.0131, Perplexity: 25.3499\n",
      "Epoch [69/100] Batch [50/63] Loss: 0.0563, Recon Loss: 0.0488, VQ Loss: 0.0074, Perplexity: 26.9087\n",
      "Epoch [69/100] Batch [60/63] Loss: 0.0672, Recon Loss: 0.0583, VQ Loss: 0.0088, Perplexity: 26.4723\n",
      "====> Epoch: 69 Average loss: 0.0674\n",
      "Epoch [70/100] Batch [0/63] Loss: 0.0747, Recon Loss: 0.0675, VQ Loss: 0.0072, Perplexity: 25.3499\n",
      "Epoch [70/100] Batch [10/63] Loss: 0.0590, Recon Loss: 0.0525, VQ Loss: 0.0065, Perplexity: 25.3499\n",
      "Epoch [70/100] Batch [20/63] Loss: 0.0597, Recon Loss: 0.0546, VQ Loss: 0.0050, Perplexity: 28.1000\n",
      "Epoch [70/100] Batch [30/63] Loss: 0.0721, Recon Loss: 0.0649, VQ Loss: 0.0072, Perplexity: 21.6681\n",
      "Epoch [70/100] Batch [40/63] Loss: 0.0605, Recon Loss: 0.0507, VQ Loss: 0.0098, Perplexity: 28.1000\n",
      "Epoch [70/100] Batch [50/63] Loss: 0.0682, Recon Loss: 0.0604, VQ Loss: 0.0078, Perplexity: 23.6292\n",
      "Epoch [70/100] Batch [60/63] Loss: 0.0647, Recon Loss: 0.0578, VQ Loss: 0.0069, Perplexity: 26.4723\n",
      "====> Epoch: 70 Average loss: 0.0671\n",
      "Epoch [71/100] Batch [0/63] Loss: 0.0703, Recon Loss: 0.0615, VQ Loss: 0.0088, Perplexity: 26.4723\n",
      "Epoch [71/100] Batch [10/63] Loss: 0.0732, Recon Loss: 0.0642, VQ Loss: 0.0089, Perplexity: 22.8690\n",
      "Epoch [71/100] Batch [20/63] Loss: 0.0604, Recon Loss: 0.0526, VQ Loss: 0.0079, Perplexity: 24.2752\n",
      "Epoch [71/100] Batch [30/63] Loss: 0.0622, Recon Loss: 0.0560, VQ Loss: 0.0063, Perplexity: 29.3441\n",
      "Epoch [71/100] Batch [40/63] Loss: 0.0613, Recon Loss: 0.0533, VQ Loss: 0.0080, Perplexity: 25.3499\n",
      "Epoch [71/100] Batch [50/63] Loss: 0.0618, Recon Loss: 0.0522, VQ Loss: 0.0096, Perplexity: 25.7678\n",
      "Epoch [71/100] Batch [60/63] Loss: 0.0746, Recon Loss: 0.0660, VQ Loss: 0.0086, Perplexity: 26.9087\n",
      "====> Epoch: 71 Average loss: 0.0672\n",
      "Epoch [72/100] Batch [0/63] Loss: 0.0635, Recon Loss: 0.0573, VQ Loss: 0.0062, Perplexity: 26.9087\n",
      "Epoch [72/100] Batch [10/63] Loss: 0.0603, Recon Loss: 0.0533, VQ Loss: 0.0070, Perplexity: 28.1000\n",
      "Epoch [72/100] Batch [20/63] Loss: 0.0661, Recon Loss: 0.0574, VQ Loss: 0.0087, Perplexity: 28.1000\n",
      "Epoch [72/100] Batch [30/63] Loss: 0.0619, Recon Loss: 0.0533, VQ Loss: 0.0085, Perplexity: 21.6681\n",
      "Epoch [72/100] Batch [40/63] Loss: 0.0657, Recon Loss: 0.0547, VQ Loss: 0.0110, Perplexity: 25.7679\n",
      "Epoch [72/100] Batch [50/63] Loss: 0.0706, Recon Loss: 0.0632, VQ Loss: 0.0073, Perplexity: 29.3441\n",
      "Epoch [72/100] Batch [60/63] Loss: 0.0705, Recon Loss: 0.0608, VQ Loss: 0.0097, Perplexity: 26.9087\n",
      "====> Epoch: 72 Average loss: 0.0671\n",
      "Epoch [73/100] Batch [0/63] Loss: 0.0631, Recon Loss: 0.0556, VQ Loss: 0.0076, Perplexity: 30.6433\n",
      "Epoch [73/100] Batch [10/63] Loss: 0.0709, Recon Loss: 0.0579, VQ Loss: 0.0130, Perplexity: 28.1000\n",
      "Epoch [73/100] Batch [20/63] Loss: 0.0692, Recon Loss: 0.0579, VQ Loss: 0.0113, Perplexity: 25.3499\n",
      "Epoch [73/100] Batch [30/63] Loss: 0.0736, Recon Loss: 0.0640, VQ Loss: 0.0095, Perplexity: 26.9087\n",
      "Epoch [73/100] Batch [40/63] Loss: 0.0661, Recon Loss: 0.0557, VQ Loss: 0.0105, Perplexity: 25.7679\n",
      "Epoch [73/100] Batch [50/63] Loss: 0.0678, Recon Loss: 0.0594, VQ Loss: 0.0084, Perplexity: 28.1000\n",
      "Epoch [73/100] Batch [60/63] Loss: 0.0600, Recon Loss: 0.0530, VQ Loss: 0.0070, Perplexity: 24.6754\n",
      "====> Epoch: 73 Average loss: 0.0671\n",
      "Epoch [74/100] Batch [0/63] Loss: 0.0639, Recon Loss: 0.0541, VQ Loss: 0.0098, Perplexity: 25.7679\n",
      "Epoch [74/100] Batch [10/63] Loss: 0.0615, Recon Loss: 0.0536, VQ Loss: 0.0079, Perplexity: 25.3499\n",
      "Epoch [74/100] Batch [20/63] Loss: 0.0638, Recon Loss: 0.0519, VQ Loss: 0.0119, Perplexity: 26.9087\n",
      "Epoch [74/100] Batch [30/63] Loss: 0.0707, Recon Loss: 0.0617, VQ Loss: 0.0090, Perplexity: 24.6754\n",
      "Epoch [74/100] Batch [40/63] Loss: 0.0594, Recon Loss: 0.0534, VQ Loss: 0.0060, Perplexity: 25.7678\n",
      "Epoch [74/100] Batch [50/63] Loss: 0.0659, Recon Loss: 0.0553, VQ Loss: 0.0106, Perplexity: 25.7678\n",
      "Epoch [74/100] Batch [60/63] Loss: 0.0594, Recon Loss: 0.0529, VQ Loss: 0.0065, Perplexity: 28.1000\n",
      "====> Epoch: 74 Average loss: 0.0669\n",
      "Epoch [75/100] Batch [0/63] Loss: 0.0595, Recon Loss: 0.0520, VQ Loss: 0.0075, Perplexity: 28.1000\n",
      "Epoch [75/100] Batch [10/63] Loss: 0.0640, Recon Loss: 0.0597, VQ Loss: 0.0043, Perplexity: 20.4129\n",
      "Epoch [75/100] Batch [20/63] Loss: 0.0688, Recon Loss: 0.0607, VQ Loss: 0.0080, Perplexity: 29.3441\n",
      "Epoch [75/100] Batch [30/63] Loss: 0.0708, Recon Loss: 0.0585, VQ Loss: 0.0123, Perplexity: 23.2460\n",
      "Epoch [75/100] Batch [40/63] Loss: 0.0631, Recon Loss: 0.0543, VQ Loss: 0.0088, Perplexity: 23.2460\n",
      "Epoch [75/100] Batch [50/63] Loss: 0.0789, Recon Loss: 0.0691, VQ Loss: 0.0098, Perplexity: 28.1000\n",
      "Epoch [75/100] Batch [60/63] Loss: 0.0638, Recon Loss: 0.0565, VQ Loss: 0.0073, Perplexity: 28.1000\n",
      "====> Epoch: 75 Average loss: 0.0670\n",
      "Epoch [76/100] Batch [0/63] Loss: 0.0713, Recon Loss: 0.0587, VQ Loss: 0.0126, Perplexity: 24.9388\n",
      "Epoch [76/100] Batch [10/63] Loss: 0.0654, Recon Loss: 0.0578, VQ Loss: 0.0075, Perplexity: 25.7678\n",
      "Epoch [76/100] Batch [20/63] Loss: 0.0621, Recon Loss: 0.0530, VQ Loss: 0.0091, Perplexity: 26.4723\n",
      "Epoch [76/100] Batch [30/63] Loss: 0.0602, Recon Loss: 0.0544, VQ Loss: 0.0059, Perplexity: 25.7678\n",
      "Epoch [76/100] Batch [40/63] Loss: 0.0757, Recon Loss: 0.0643, VQ Loss: 0.0114, Perplexity: 26.9087\n",
      "Epoch [76/100] Batch [50/63] Loss: 0.0624, Recon Loss: 0.0535, VQ Loss: 0.0089, Perplexity: 24.2752\n",
      "Epoch [76/100] Batch [60/63] Loss: 0.0731, Recon Loss: 0.0621, VQ Loss: 0.0110, Perplexity: 29.3441\n",
      "====> Epoch: 76 Average loss: 0.0670\n",
      "Epoch [77/100] Batch [0/63] Loss: 0.0669, Recon Loss: 0.0529, VQ Loss: 0.0140, Perplexity: 25.3499\n",
      "Epoch [77/100] Batch [10/63] Loss: 0.0726, Recon Loss: 0.0646, VQ Loss: 0.0079, Perplexity: 28.1000\n",
      "Epoch [77/100] Batch [20/63] Loss: 0.0674, Recon Loss: 0.0594, VQ Loss: 0.0080, Perplexity: 30.6433\n",
      "Epoch [77/100] Batch [30/63] Loss: 0.0705, Recon Loss: 0.0643, VQ Loss: 0.0061, Perplexity: 28.1000\n",
      "Epoch [77/100] Batch [40/63] Loss: 0.0553, Recon Loss: 0.0470, VQ Loss: 0.0083, Perplexity: 25.7678\n",
      "Epoch [77/100] Batch [50/63] Loss: 0.0604, Recon Loss: 0.0525, VQ Loss: 0.0079, Perplexity: 20.9709\n",
      "Epoch [77/100] Batch [60/63] Loss: 0.0713, Recon Loss: 0.0617, VQ Loss: 0.0096, Perplexity: 24.2752\n",
      "====> Epoch: 77 Average loss: 0.0669\n",
      "Epoch [78/100] Batch [0/63] Loss: 0.0670, Recon Loss: 0.0599, VQ Loss: 0.0071, Perplexity: 24.6754\n",
      "Epoch [78/100] Batch [10/63] Loss: 0.0651, Recon Loss: 0.0534, VQ Loss: 0.0117, Perplexity: 25.7678\n",
      "Epoch [78/100] Batch [20/63] Loss: 0.0758, Recon Loss: 0.0629, VQ Loss: 0.0129, Perplexity: 25.3499\n",
      "Epoch [78/100] Batch [30/63] Loss: 0.0720, Recon Loss: 0.0620, VQ Loss: 0.0100, Perplexity: 30.6433\n",
      "Epoch [78/100] Batch [40/63] Loss: 0.0645, Recon Loss: 0.0575, VQ Loss: 0.0069, Perplexity: 20.4129\n",
      "Epoch [78/100] Batch [50/63] Loss: 0.0654, Recon Loss: 0.0547, VQ Loss: 0.0106, Perplexity: 22.6274\n",
      "Epoch [78/100] Batch [60/63] Loss: 0.0628, Recon Loss: 0.0564, VQ Loss: 0.0064, Perplexity: 26.9087\n",
      "====> Epoch: 78 Average loss: 0.0668\n",
      "Epoch [79/100] Batch [0/63] Loss: 0.0684, Recon Loss: 0.0558, VQ Loss: 0.0126, Perplexity: 25.7678\n",
      "Epoch [79/100] Batch [10/63] Loss: 0.0654, Recon Loss: 0.0562, VQ Loss: 0.0092, Perplexity: 26.9087\n",
      "Epoch [79/100] Batch [20/63] Loss: 0.0727, Recon Loss: 0.0674, VQ Loss: 0.0052, Perplexity: 28.1000\n",
      "Epoch [79/100] Batch [30/63] Loss: 0.0591, Recon Loss: 0.0546, VQ Loss: 0.0045, Perplexity: 26.4723\n",
      "Epoch [79/100] Batch [40/63] Loss: 0.0686, Recon Loss: 0.0602, VQ Loss: 0.0084, Perplexity: 29.3441\n",
      "Epoch [79/100] Batch [50/63] Loss: 0.0709, Recon Loss: 0.0610, VQ Loss: 0.0099, Perplexity: 26.4723\n",
      "Epoch [79/100] Batch [60/63] Loss: 0.0659, Recon Loss: 0.0555, VQ Loss: 0.0104, Perplexity: 24.2752\n",
      "====> Epoch: 79 Average loss: 0.0668\n",
      "Epoch [80/100] Batch [0/63] Loss: 0.0641, Recon Loss: 0.0562, VQ Loss: 0.0079, Perplexity: 25.3499\n",
      "Epoch [80/100] Batch [10/63] Loss: 0.0630, Recon Loss: 0.0507, VQ Loss: 0.0123, Perplexity: 26.9087\n",
      "Epoch [80/100] Batch [20/63] Loss: 0.0647, Recon Loss: 0.0539, VQ Loss: 0.0108, Perplexity: 28.1000\n",
      "Epoch [80/100] Batch [30/63] Loss: 0.0678, Recon Loss: 0.0592, VQ Loss: 0.0086, Perplexity: 28.1000\n",
      "Epoch [80/100] Batch [40/63] Loss: 0.0749, Recon Loss: 0.0629, VQ Loss: 0.0120, Perplexity: 28.1000\n",
      "Epoch [80/100] Batch [50/63] Loss: 0.0600, Recon Loss: 0.0538, VQ Loss: 0.0062, Perplexity: 22.2604\n",
      "Epoch [80/100] Batch [60/63] Loss: 0.0613, Recon Loss: 0.0517, VQ Loss: 0.0096, Perplexity: 25.3499\n",
      "====> Epoch: 80 Average loss: 0.0668\n",
      "Epoch [81/100] Batch [0/63] Loss: 0.0573, Recon Loss: 0.0492, VQ Loss: 0.0081, Perplexity: 28.1000\n",
      "Epoch [81/100] Batch [10/63] Loss: 0.0591, Recon Loss: 0.0510, VQ Loss: 0.0081, Perplexity: 25.7678\n",
      "Epoch [81/100] Batch [20/63] Loss: 0.0742, Recon Loss: 0.0617, VQ Loss: 0.0125, Perplexity: 26.9087\n",
      "Epoch [81/100] Batch [30/63] Loss: 0.0694, Recon Loss: 0.0613, VQ Loss: 0.0081, Perplexity: 29.3441\n",
      "Epoch [81/100] Batch [40/63] Loss: 0.0677, Recon Loss: 0.0603, VQ Loss: 0.0074, Perplexity: 26.9087\n",
      "Epoch [81/100] Batch [50/63] Loss: 0.0729, Recon Loss: 0.0644, VQ Loss: 0.0084, Perplexity: 24.2752\n",
      "Epoch [81/100] Batch [60/63] Loss: 0.0683, Recon Loss: 0.0617, VQ Loss: 0.0065, Perplexity: 20.9709\n",
      "====> Epoch: 81 Average loss: 0.0667\n",
      "Epoch [82/100] Batch [0/63] Loss: 0.0699, Recon Loss: 0.0623, VQ Loss: 0.0077, Perplexity: 26.9087\n",
      "Epoch [82/100] Batch [10/63] Loss: 0.0591, Recon Loss: 0.0534, VQ Loss: 0.0057, Perplexity: 26.9087\n",
      "Epoch [82/100] Batch [20/63] Loss: 0.0740, Recon Loss: 0.0599, VQ Loss: 0.0140, Perplexity: 25.3499\n",
      "Epoch [82/100] Batch [30/63] Loss: 0.0621, Recon Loss: 0.0564, VQ Loss: 0.0056, Perplexity: 22.6274\n",
      "Epoch [82/100] Batch [40/63] Loss: 0.0707, Recon Loss: 0.0612, VQ Loss: 0.0095, Perplexity: 29.3441\n",
      "Epoch [82/100] Batch [50/63] Loss: 0.0648, Recon Loss: 0.0584, VQ Loss: 0.0064, Perplexity: 24.6754\n",
      "Epoch [82/100] Batch [60/63] Loss: 0.0679, Recon Loss: 0.0610, VQ Loss: 0.0068, Perplexity: 25.7678\n",
      "====> Epoch: 82 Average loss: 0.0668\n",
      "Epoch [83/100] Batch [0/63] Loss: 0.0615, Recon Loss: 0.0522, VQ Loss: 0.0094, Perplexity: 26.9087\n",
      "Epoch [83/100] Batch [10/63] Loss: 0.0601, Recon Loss: 0.0531, VQ Loss: 0.0070, Perplexity: 26.4723\n",
      "Epoch [83/100] Batch [20/63] Loss: 0.0629, Recon Loss: 0.0548, VQ Loss: 0.0081, Perplexity: 24.6754\n",
      "Epoch [83/100] Batch [30/63] Loss: 0.0815, Recon Loss: 0.0730, VQ Loss: 0.0085, Perplexity: 25.7678\n",
      "Epoch [83/100] Batch [40/63] Loss: 0.0624, Recon Loss: 0.0556, VQ Loss: 0.0068, Perplexity: 30.6433\n",
      "Epoch [83/100] Batch [50/63] Loss: 0.0635, Recon Loss: 0.0543, VQ Loss: 0.0092, Perplexity: 28.1000\n",
      "Epoch [83/100] Batch [60/63] Loss: 0.0594, Recon Loss: 0.0521, VQ Loss: 0.0074, Perplexity: 24.6754\n",
      "====> Epoch: 83 Average loss: 0.0666\n",
      "Epoch [84/100] Batch [0/63] Loss: 0.0625, Recon Loss: 0.0560, VQ Loss: 0.0065, Perplexity: 28.1000\n",
      "Epoch [84/100] Batch [10/63] Loss: 0.0637, Recon Loss: 0.0555, VQ Loss: 0.0082, Perplexity: 25.7678\n",
      "Epoch [84/100] Batch [20/63] Loss: 0.0550, Recon Loss: 0.0463, VQ Loss: 0.0087, Perplexity: 23.6292\n",
      "Epoch [84/100] Batch [30/63] Loss: 0.0667, Recon Loss: 0.0548, VQ Loss: 0.0119, Perplexity: 29.3441\n",
      "Epoch [84/100] Batch [40/63] Loss: 0.0638, Recon Loss: 0.0549, VQ Loss: 0.0089, Perplexity: 26.9087\n",
      "Epoch [84/100] Batch [50/63] Loss: 0.0669, Recon Loss: 0.0607, VQ Loss: 0.0062, Perplexity: 23.2460\n",
      "Epoch [84/100] Batch [60/63] Loss: 0.0575, Recon Loss: 0.0505, VQ Loss: 0.0070, Perplexity: 25.7678\n",
      "====> Epoch: 84 Average loss: 0.0666\n",
      "Epoch [85/100] Batch [0/63] Loss: 0.0689, Recon Loss: 0.0601, VQ Loss: 0.0088, Perplexity: 23.2460\n",
      "Epoch [85/100] Batch [10/63] Loss: 0.0746, Recon Loss: 0.0643, VQ Loss: 0.0103, Perplexity: 28.1000\n",
      "Epoch [85/100] Batch [20/63] Loss: 0.0589, Recon Loss: 0.0520, VQ Loss: 0.0070, Perplexity: 26.4723\n",
      "Epoch [85/100] Batch [30/63] Loss: 0.0733, Recon Loss: 0.0648, VQ Loss: 0.0086, Perplexity: 29.3441\n",
      "Epoch [85/100] Batch [40/63] Loss: 0.0563, Recon Loss: 0.0498, VQ Loss: 0.0065, Perplexity: 26.9087\n",
      "Epoch [85/100] Batch [50/63] Loss: 0.0685, Recon Loss: 0.0603, VQ Loss: 0.0081, Perplexity: 24.2752\n",
      "Epoch [85/100] Batch [60/63] Loss: 0.0635, Recon Loss: 0.0538, VQ Loss: 0.0098, Perplexity: 24.6754\n",
      "====> Epoch: 85 Average loss: 0.0666\n",
      "Epoch [86/100] Batch [0/63] Loss: 0.0628, Recon Loss: 0.0571, VQ Loss: 0.0057, Perplexity: 22.8690\n",
      "Epoch [86/100] Batch [10/63] Loss: 0.0775, Recon Loss: 0.0708, VQ Loss: 0.0066, Perplexity: 28.1000\n",
      "Epoch [86/100] Batch [20/63] Loss: 0.0719, Recon Loss: 0.0630, VQ Loss: 0.0089, Perplexity: 25.7678\n",
      "Epoch [86/100] Batch [30/63] Loss: 0.0589, Recon Loss: 0.0515, VQ Loss: 0.0074, Perplexity: 25.7679\n",
      "Epoch [86/100] Batch [40/63] Loss: 0.0622, Recon Loss: 0.0535, VQ Loss: 0.0087, Perplexity: 27.6443\n",
      "Epoch [86/100] Batch [50/63] Loss: 0.0669, Recon Loss: 0.0601, VQ Loss: 0.0068, Perplexity: 25.3499\n",
      "Epoch [86/100] Batch [60/63] Loss: 0.0704, Recon Loss: 0.0622, VQ Loss: 0.0081, Perplexity: 25.7679\n",
      "====> Epoch: 86 Average loss: 0.0664\n",
      "Epoch [87/100] Batch [0/63] Loss: 0.0668, Recon Loss: 0.0539, VQ Loss: 0.0130, Perplexity: 26.9087\n",
      "Epoch [87/100] Batch [10/63] Loss: 0.0731, Recon Loss: 0.0652, VQ Loss: 0.0079, Perplexity: 26.9087\n",
      "Epoch [87/100] Batch [20/63] Loss: 0.0607, Recon Loss: 0.0522, VQ Loss: 0.0084, Perplexity: 24.6754\n",
      "Epoch [87/100] Batch [30/63] Loss: 0.0736, Recon Loss: 0.0613, VQ Loss: 0.0122, Perplexity: 25.7678\n",
      "Epoch [87/100] Batch [40/63] Loss: 0.0734, Recon Loss: 0.0676, VQ Loss: 0.0058, Perplexity: 27.6443\n",
      "Epoch [87/100] Batch [50/63] Loss: 0.0680, Recon Loss: 0.0571, VQ Loss: 0.0109, Perplexity: 24.6754\n",
      "Epoch [87/100] Batch [60/63] Loss: 0.0665, Recon Loss: 0.0587, VQ Loss: 0.0078, Perplexity: 22.8690\n",
      "====> Epoch: 87 Average loss: 0.0666\n",
      "Epoch [88/100] Batch [0/63] Loss: 0.0770, Recon Loss: 0.0699, VQ Loss: 0.0071, Perplexity: 23.6292\n",
      "Epoch [88/100] Batch [10/63] Loss: 0.0735, Recon Loss: 0.0637, VQ Loss: 0.0098, Perplexity: 25.7678\n",
      "Epoch [88/100] Batch [20/63] Loss: 0.0543, Recon Loss: 0.0480, VQ Loss: 0.0064, Perplexity: 26.4723\n",
      "Epoch [88/100] Batch [30/63] Loss: 0.0660, Recon Loss: 0.0577, VQ Loss: 0.0083, Perplexity: 25.3499\n",
      "Epoch [88/100] Batch [40/63] Loss: 0.0600, Recon Loss: 0.0540, VQ Loss: 0.0060, Perplexity: 26.9087\n",
      "Epoch [88/100] Batch [50/63] Loss: 0.0587, Recon Loss: 0.0507, VQ Loss: 0.0080, Perplexity: 24.2752\n",
      "Epoch [88/100] Batch [60/63] Loss: 0.0671, Recon Loss: 0.0560, VQ Loss: 0.0112, Perplexity: 25.7678\n",
      "====> Epoch: 88 Average loss: 0.0664\n",
      "Epoch [89/100] Batch [0/63] Loss: 0.0583, Recon Loss: 0.0528, VQ Loss: 0.0055, Perplexity: 25.3499\n",
      "Epoch [89/100] Batch [10/63] Loss: 0.0635, Recon Loss: 0.0585, VQ Loss: 0.0050, Perplexity: 24.9388\n",
      "Epoch [89/100] Batch [20/63] Loss: 0.0658, Recon Loss: 0.0566, VQ Loss: 0.0091, Perplexity: 25.7678\n",
      "Epoch [89/100] Batch [30/63] Loss: 0.0784, Recon Loss: 0.0676, VQ Loss: 0.0109, Perplexity: 29.3441\n",
      "Epoch [89/100] Batch [40/63] Loss: 0.0656, Recon Loss: 0.0568, VQ Loss: 0.0088, Perplexity: 26.9087\n",
      "Epoch [89/100] Batch [50/63] Loss: 0.0708, Recon Loss: 0.0605, VQ Loss: 0.0103, Perplexity: 26.9087\n",
      "Epoch [89/100] Batch [60/63] Loss: 0.0677, Recon Loss: 0.0609, VQ Loss: 0.0068, Perplexity: 24.6754\n",
      "====> Epoch: 89 Average loss: 0.0665\n",
      "Epoch [90/100] Batch [0/63] Loss: 0.0612, Recon Loss: 0.0514, VQ Loss: 0.0098, Perplexity: 27.6443\n",
      "Epoch [90/100] Batch [10/63] Loss: 0.0647, Recon Loss: 0.0547, VQ Loss: 0.0100, Perplexity: 24.6754\n",
      "Epoch [90/100] Batch [20/63] Loss: 0.0585, Recon Loss: 0.0529, VQ Loss: 0.0056, Perplexity: 26.9087\n",
      "Epoch [90/100] Batch [30/63] Loss: 0.0619, Recon Loss: 0.0545, VQ Loss: 0.0075, Perplexity: 28.1000\n",
      "Epoch [90/100] Batch [40/63] Loss: 0.0593, Recon Loss: 0.0546, VQ Loss: 0.0048, Perplexity: 26.9087\n",
      "Epoch [90/100] Batch [50/63] Loss: 0.0606, Recon Loss: 0.0526, VQ Loss: 0.0081, Perplexity: 26.4723\n",
      "Epoch [90/100] Batch [60/63] Loss: 0.0667, Recon Loss: 0.0587, VQ Loss: 0.0081, Perplexity: 26.9087\n",
      "====> Epoch: 90 Average loss: 0.0664\n",
      "Epoch [91/100] Batch [0/63] Loss: 0.0648, Recon Loss: 0.0581, VQ Loss: 0.0067, Perplexity: 28.1000\n",
      "Epoch [91/100] Batch [10/63] Loss: 0.0669, Recon Loss: 0.0558, VQ Loss: 0.0112, Perplexity: 27.6443\n",
      "Epoch [91/100] Batch [20/63] Loss: 0.0617, Recon Loss: 0.0537, VQ Loss: 0.0079, Perplexity: 22.8690\n",
      "Epoch [91/100] Batch [30/63] Loss: 0.0620, Recon Loss: 0.0513, VQ Loss: 0.0108, Perplexity: 26.9087\n",
      "Epoch [91/100] Batch [40/63] Loss: 0.0734, Recon Loss: 0.0661, VQ Loss: 0.0073, Perplexity: 29.3441\n",
      "Epoch [91/100] Batch [50/63] Loss: 0.0626, Recon Loss: 0.0542, VQ Loss: 0.0084, Perplexity: 26.9087\n",
      "Epoch [91/100] Batch [60/63] Loss: 0.0608, Recon Loss: 0.0536, VQ Loss: 0.0072, Perplexity: 24.2752\n",
      "====> Epoch: 91 Average loss: 0.0664\n",
      "Epoch [92/100] Batch [0/63] Loss: 0.0616, Recon Loss: 0.0549, VQ Loss: 0.0067, Perplexity: 24.2752\n",
      "Epoch [92/100] Batch [10/63] Loss: 0.0656, Recon Loss: 0.0572, VQ Loss: 0.0083, Perplexity: 25.7678\n",
      "Epoch [92/100] Batch [20/63] Loss: 0.0659, Recon Loss: 0.0604, VQ Loss: 0.0055, Perplexity: 28.1000\n",
      "Epoch [92/100] Batch [30/63] Loss: 0.0561, Recon Loss: 0.0489, VQ Loss: 0.0072, Perplexity: 28.1000\n",
      "Epoch [92/100] Batch [40/63] Loss: 0.0672, Recon Loss: 0.0599, VQ Loss: 0.0072, Perplexity: 28.1000\n",
      "Epoch [92/100] Batch [50/63] Loss: 0.0677, Recon Loss: 0.0618, VQ Loss: 0.0059, Perplexity: 25.3499\n",
      "Epoch [92/100] Batch [60/63] Loss: 0.0662, Recon Loss: 0.0608, VQ Loss: 0.0054, Perplexity: 29.3441\n",
      "====> Epoch: 92 Average loss: 0.0663\n",
      "Epoch [93/100] Batch [0/63] Loss: 0.0642, Recon Loss: 0.0559, VQ Loss: 0.0083, Perplexity: 25.7678\n",
      "Epoch [93/100] Batch [10/63] Loss: 0.0751, Recon Loss: 0.0650, VQ Loss: 0.0102, Perplexity: 27.6443\n",
      "Epoch [93/100] Batch [20/63] Loss: 0.0795, Recon Loss: 0.0707, VQ Loss: 0.0088, Perplexity: 23.8815\n",
      "Epoch [93/100] Batch [30/63] Loss: 0.0658, Recon Loss: 0.0596, VQ Loss: 0.0062, Perplexity: 28.1000\n",
      "Epoch [93/100] Batch [40/63] Loss: 0.0776, Recon Loss: 0.0649, VQ Loss: 0.0127, Perplexity: 28.1000\n",
      "Epoch [93/100] Batch [50/63] Loss: 0.0741, Recon Loss: 0.0675, VQ Loss: 0.0066, Perplexity: 26.9087\n",
      "Epoch [93/100] Batch [60/63] Loss: 0.0635, Recon Loss: 0.0571, VQ Loss: 0.0064, Perplexity: 21.8994\n",
      "====> Epoch: 93 Average loss: 0.0664\n",
      "Epoch [94/100] Batch [0/63] Loss: 0.0633, Recon Loss: 0.0562, VQ Loss: 0.0071, Perplexity: 22.6274\n",
      "Epoch [94/100] Batch [10/63] Loss: 0.0644, Recon Loss: 0.0571, VQ Loss: 0.0073, Perplexity: 26.9087\n",
      "Epoch [94/100] Batch [20/63] Loss: 0.0588, Recon Loss: 0.0505, VQ Loss: 0.0084, Perplexity: 26.4723\n",
      "Epoch [94/100] Batch [30/63] Loss: 0.0604, Recon Loss: 0.0504, VQ Loss: 0.0100, Perplexity: 24.2752\n",
      "Epoch [94/100] Batch [40/63] Loss: 0.0792, Recon Loss: 0.0706, VQ Loss: 0.0085, Perplexity: 26.9087\n",
      "Epoch [94/100] Batch [50/63] Loss: 0.0668, Recon Loss: 0.0600, VQ Loss: 0.0068, Perplexity: 29.3441\n",
      "Epoch [94/100] Batch [60/63] Loss: 0.0682, Recon Loss: 0.0594, VQ Loss: 0.0087, Perplexity: 28.8682\n",
      "====> Epoch: 94 Average loss: 0.0664\n",
      "Epoch [95/100] Batch [0/63] Loss: 0.0641, Recon Loss: 0.0542, VQ Loss: 0.0099, Perplexity: 26.9087\n",
      "Epoch [95/100] Batch [10/63] Loss: 0.0641, Recon Loss: 0.0556, VQ Loss: 0.0085, Perplexity: 28.1000\n",
      "Epoch [95/100] Batch [20/63] Loss: 0.0598, Recon Loss: 0.0521, VQ Loss: 0.0077, Perplexity: 29.3441\n",
      "Epoch [95/100] Batch [30/63] Loss: 0.0713, Recon Loss: 0.0650, VQ Loss: 0.0063, Perplexity: 23.6292\n",
      "Epoch [95/100] Batch [40/63] Loss: 0.0649, Recon Loss: 0.0560, VQ Loss: 0.0089, Perplexity: 26.9087\n",
      "Epoch [95/100] Batch [50/63] Loss: 0.0588, Recon Loss: 0.0494, VQ Loss: 0.0094, Perplexity: 20.9709\n",
      "Epoch [95/100] Batch [60/63] Loss: 0.0725, Recon Loss: 0.0660, VQ Loss: 0.0064, Perplexity: 26.9087\n",
      "====> Epoch: 95 Average loss: 0.0664\n",
      "Epoch [96/100] Batch [0/63] Loss: 0.0605, Recon Loss: 0.0560, VQ Loss: 0.0045, Perplexity: 25.3499\n",
      "Epoch [96/100] Batch [10/63] Loss: 0.0609, Recon Loss: 0.0525, VQ Loss: 0.0083, Perplexity: 26.9087\n",
      "Epoch [96/100] Batch [20/63] Loss: 0.0646, Recon Loss: 0.0576, VQ Loss: 0.0070, Perplexity: 26.9087\n",
      "Epoch [96/100] Batch [30/63] Loss: 0.0614, Recon Loss: 0.0556, VQ Loss: 0.0057, Perplexity: 29.3441\n",
      "Epoch [96/100] Batch [40/63] Loss: 0.0696, Recon Loss: 0.0625, VQ Loss: 0.0072, Perplexity: 22.2604\n",
      "Epoch [96/100] Batch [50/63] Loss: 0.0617, Recon Loss: 0.0512, VQ Loss: 0.0105, Perplexity: 29.3441\n",
      "Epoch [96/100] Batch [60/63] Loss: 0.0649, Recon Loss: 0.0580, VQ Loss: 0.0069, Perplexity: 23.6292\n",
      "====> Epoch: 96 Average loss: 0.0663\n",
      "Epoch [97/100] Batch [0/63] Loss: 0.0600, Recon Loss: 0.0525, VQ Loss: 0.0075, Perplexity: 26.9087\n",
      "Epoch [97/100] Batch [10/63] Loss: 0.0701, Recon Loss: 0.0597, VQ Loss: 0.0104, Perplexity: 30.6433\n",
      "Epoch [97/100] Batch [20/63] Loss: 0.0658, Recon Loss: 0.0577, VQ Loss: 0.0081, Perplexity: 23.2460\n",
      "Epoch [97/100] Batch [30/63] Loss: 0.0627, Recon Loss: 0.0543, VQ Loss: 0.0083, Perplexity: 24.2752\n",
      "Epoch [97/100] Batch [40/63] Loss: 0.0647, Recon Loss: 0.0574, VQ Loss: 0.0073, Perplexity: 25.7678\n",
      "Epoch [97/100] Batch [50/63] Loss: 0.0631, Recon Loss: 0.0547, VQ Loss: 0.0084, Perplexity: 22.8690\n",
      "Epoch [97/100] Batch [60/63] Loss: 0.0638, Recon Loss: 0.0570, VQ Loss: 0.0068, Perplexity: 27.6443\n",
      "====> Epoch: 97 Average loss: 0.0662\n",
      "Epoch [98/100] Batch [0/63] Loss: 0.0742, Recon Loss: 0.0624, VQ Loss: 0.0117, Perplexity: 26.9087\n",
      "Epoch [98/100] Batch [10/63] Loss: 0.0704, Recon Loss: 0.0614, VQ Loss: 0.0090, Perplexity: 29.3441\n",
      "Epoch [98/100] Batch [20/63] Loss: 0.0694, Recon Loss: 0.0602, VQ Loss: 0.0093, Perplexity: 24.6754\n",
      "Epoch [98/100] Batch [30/63] Loss: 0.0569, Recon Loss: 0.0494, VQ Loss: 0.0075, Perplexity: 25.7678\n",
      "Epoch [98/100] Batch [40/63] Loss: 0.0741, Recon Loss: 0.0643, VQ Loss: 0.0098, Perplexity: 26.9087\n",
      "Epoch [98/100] Batch [50/63] Loss: 0.0672, Recon Loss: 0.0573, VQ Loss: 0.0098, Perplexity: 22.8690\n",
      "Epoch [98/100] Batch [60/63] Loss: 0.0689, Recon Loss: 0.0609, VQ Loss: 0.0081, Perplexity: 29.3441\n",
      "====> Epoch: 98 Average loss: 0.0663\n",
      "Epoch [99/100] Batch [0/63] Loss: 0.0537, Recon Loss: 0.0456, VQ Loss: 0.0081, Perplexity: 30.6433\n",
      "Epoch [99/100] Batch [10/63] Loss: 0.0719, Recon Loss: 0.0616, VQ Loss: 0.0102, Perplexity: 22.6274\n",
      "Epoch [99/100] Batch [20/63] Loss: 0.0738, Recon Loss: 0.0660, VQ Loss: 0.0079, Perplexity: 24.6754\n",
      "Epoch [99/100] Batch [30/63] Loss: 0.0673, Recon Loss: 0.0580, VQ Loss: 0.0092, Perplexity: 24.6754\n",
      "Epoch [99/100] Batch [40/63] Loss: 0.0576, Recon Loss: 0.0518, VQ Loss: 0.0058, Perplexity: 26.9087\n",
      "Epoch [99/100] Batch [50/63] Loss: 0.0616, Recon Loss: 0.0523, VQ Loss: 0.0094, Perplexity: 21.8521\n",
      "Epoch [99/100] Batch [60/63] Loss: 0.0659, Recon Loss: 0.0575, VQ Loss: 0.0084, Perplexity: 29.3441\n",
      "====> Epoch: 99 Average loss: 0.0663\n",
      "Epoch [100/100] Batch [0/63] Loss: 0.0675, Recon Loss: 0.0553, VQ Loss: 0.0122, Perplexity: 29.3441\n",
      "Epoch [100/100] Batch [10/63] Loss: 0.0660, Recon Loss: 0.0561, VQ Loss: 0.0099, Perplexity: 25.7678\n",
      "Epoch [100/100] Batch [20/63] Loss: 0.0708, Recon Loss: 0.0652, VQ Loss: 0.0057, Perplexity: 25.7678\n",
      "Epoch [100/100] Batch [30/63] Loss: 0.0699, Recon Loss: 0.0572, VQ Loss: 0.0127, Perplexity: 26.4723\n",
      "Epoch [100/100] Batch [40/63] Loss: 0.0611, Recon Loss: 0.0542, VQ Loss: 0.0069, Perplexity: 22.8690\n",
      "Epoch [100/100] Batch [50/63] Loss: 0.0607, Recon Loss: 0.0521, VQ Loss: 0.0086, Perplexity: 24.2752\n",
      "Epoch [100/100] Batch [60/63] Loss: 0.0743, Recon Loss: 0.0665, VQ Loss: 0.0078, Perplexity: 25.7678\n",
      "====> Epoch: 100 Average loss: 0.0663\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "input_dim = data.shape[1]  # Should be 50 for your dataset\n",
    "hidden_dims = [128, 64]\n",
    "embedding_dim = 5          \n",
    "num_embeddings = 128\n",
    "commitment_cost = 0.25\n",
    "\n",
    "# Instantiate the model with embedding_dim set to 5\n",
    "model = VQVAE(input_dim, hidden_dims, embedding_dim, num_embeddings, commitment_cost)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "log_interval = 10\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data_batch,) in enumerate(data_loader):\n",
    "        data_batch = data_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        vq_loss, recon_batch, perplexity = model(data_batch)\n",
    "        \n",
    "        recon_loss = ((data_batch - recon_batch) ** 2).mean()\n",
    "        loss = recon_loss + vq_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}] Batch [{batch_idx}/{len(data_loader)}] '\n",
    "                  f'Loss: {loss.item():.4f}, Recon Loss: {recon_loss.item():.4f}, '\n",
    "                  f'VQ Loss: {vq_loss.item():.4f}, Perplexity: {perplexity.item():.4f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(data_loader)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss:.4f}')\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), 'vqvae_model.pth')\n",
    "# print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815d0bd-6e64-4b72-b78e-c88b25a6e571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
